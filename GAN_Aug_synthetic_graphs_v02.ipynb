{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d38fca4-d28a-4d2e-ae47-3547fe38489e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¬ COMPLETE GRAPH GENERATION PIPELINE\n",
      "============================================================\n",
      "Output directory: D:\\PhD\\Chapter_4\\Code2\\pdbbind\\complete_graphs_20250709_163209\n",
      "\n",
      "ðŸ“Š Loading datasets...\n",
      "Train: 9662, Val: 903, Core: 257, Holdout: 3393\n",
      "Using device: cuda\n",
      "\n",
      "============================== P ==============================\n",
      "ðŸ”§ Generating real graphs from JSON files...\n",
      "  Processing training: 9662 entries for P...\n",
      "  training P: 9312 real graphs saved, 350 failed\n",
      "  Processing validation: 903 entries for P...\n",
      "  validation P: 871 real graphs saved, 32 failed\n",
      "  Processing core: 257 entries for P...\n",
      "  core P: 249 real graphs saved, 8 failed\n",
      "  Processing holdout: 3393 entries for P...\n",
      "  holdout P: 3232 real graphs saved, 161 failed\n",
      "ðŸ¤– Generating synthetic graphs using augmentation...\n",
      "  Generating synthetic graphs for P using Controlled Augmentation...\n",
      "    Found 9312 PDB directories\n",
      "    Progress: 0/9312\n",
      "    Progress: 500/9312\n",
      "    Progress: 1000/9312\n",
      "    Progress: 1500/9312\n",
      "    Progress: 2000/9312\n",
      "    Progress: 2500/9312\n",
      "    Progress: 3000/9312\n",
      "    Progress: 3500/9312\n",
      "    Progress: 4000/9312\n",
      "    Progress: 4500/9312\n",
      "    Progress: 5000/9312\n",
      "    Progress: 5500/9312\n",
      "    Progress: 6000/9312\n",
      "    Progress: 6500/9312\n",
      "    Progress: 7000/9312\n",
      "    Progress: 7500/9312\n",
      "    Progress: 8000/9312\n",
      "    Progress: 8500/9312\n",
      "    Progress: 9000/9312\n",
      "  P: 9287 synthetic graphs generated, 25 failed\n",
      "    Failure breakdown: {'no_graph': 0, 'no_affinity': 0, 'generation_failed': 0, 'empty_result': 25}\n",
      "  Generating synthetic graphs for P using Controlled Augmentation...\n",
      "    Found 871 PDB directories\n",
      "    Progress: 0/871\n",
      "    Progress: 500/871\n",
      "  P: 871 synthetic graphs generated, 0 failed\n",
      "    Failure breakdown: {'no_graph': 0, 'no_affinity': 0, 'generation_failed': 0, 'empty_result': 0}\n",
      "\n",
      "============================== L ==============================\n",
      "ðŸ”§ Generating real graphs from JSON files...\n",
      "  Processing training: 9662 entries for L...\n",
      "  training L: 9312 real graphs saved, 350 failed\n",
      "  Processing validation: 903 entries for L...\n",
      "  validation L: 871 real graphs saved, 32 failed\n",
      "  Processing core: 257 entries for L...\n",
      "  core L: 249 real graphs saved, 8 failed\n",
      "  Processing holdout: 3393 entries for L...\n",
      "  holdout L: 3232 real graphs saved, 161 failed\n",
      "ðŸ¤– Generating synthetic graphs using augmentation...\n",
      "  Generating synthetic graphs for L using Controlled Augmentation...\n",
      "    Found 9312 PDB directories\n",
      "    Progress: 0/9312\n",
      "    Progress: 500/9312\n",
      "    Progress: 1000/9312\n",
      "    Progress: 1500/9312\n",
      "    Progress: 2000/9312\n",
      "    Progress: 2500/9312\n",
      "    Progress: 3000/9312\n",
      "    Progress: 3500/9312\n",
      "    Progress: 4000/9312\n",
      "    Progress: 4500/9312\n",
      "    Progress: 5000/9312\n",
      "    Progress: 5500/9312\n",
      "    Progress: 6000/9312\n",
      "    Progress: 6500/9312\n",
      "    Progress: 7000/9312\n",
      "    Progress: 7500/9312\n",
      "    Progress: 8000/9312\n",
      "    Progress: 8500/9312\n",
      "    Progress: 9000/9312\n",
      "  L: 9312 synthetic graphs generated, 0 failed\n",
      "    Failure breakdown: {'no_graph': 0, 'no_affinity': 0, 'generation_failed': 0, 'empty_result': 0}\n",
      "  Generating synthetic graphs for L using Controlled Augmentation...\n",
      "    Found 871 PDB directories\n",
      "    Progress: 0/871\n",
      "    Progress: 500/871\n",
      "  L: 871 synthetic graphs generated, 0 failed\n",
      "    Failure breakdown: {'no_graph': 0, 'no_affinity': 0, 'generation_failed': 0, 'empty_result': 0}\n",
      "\n",
      "============================== I ==============================\n",
      "ðŸ”§ Generating real graphs from JSON files...\n",
      "  Processing training: 9662 entries for I...\n",
      "  training I: 9312 real graphs saved, 350 failed\n",
      "  Processing validation: 903 entries for I...\n",
      "  validation I: 871 real graphs saved, 32 failed\n",
      "  Processing core: 257 entries for I...\n",
      "  core I: 249 real graphs saved, 8 failed\n",
      "  Processing holdout: 3393 entries for I...\n",
      "  holdout I: 3232 real graphs saved, 161 failed\n",
      "ðŸ¤– Generating synthetic graphs using augmentation...\n",
      "  Generating synthetic graphs for I using Controlled Augmentation...\n",
      "    Found 9312 PDB directories\n",
      "    Progress: 0/9312\n",
      "    Progress: 500/9312\n",
      "    Progress: 1000/9312\n",
      "    Progress: 1500/9312\n",
      "    Progress: 2000/9312\n",
      "    Progress: 2500/9312\n",
      "    Progress: 3000/9312\n",
      "    Progress: 3500/9312\n",
      "    Progress: 4000/9312\n",
      "    Progress: 4500/9312\n",
      "    Progress: 5000/9312\n",
      "    Progress: 5500/9312\n",
      "    Progress: 6000/9312\n",
      "    Progress: 6500/9312\n",
      "    Progress: 7000/9312\n",
      "    Progress: 7500/9312\n",
      "    Progress: 8000/9312\n",
      "    Progress: 8500/9312\n",
      "    Progress: 9000/9312\n",
      "  I: 9312 synthetic graphs generated, 0 failed\n",
      "    Failure breakdown: {'no_graph': 0, 'no_affinity': 0, 'generation_failed': 0, 'empty_result': 0}\n",
      "  Generating synthetic graphs for I using Controlled Augmentation...\n",
      "    Found 871 PDB directories\n",
      "    Progress: 0/871\n",
      "    Progress: 500/871\n",
      "  I: 871 synthetic graphs generated, 0 failed\n",
      "    Failure breakdown: {'no_graph': 0, 'no_affinity': 0, 'generation_failed': 0, 'empty_result': 0}\n",
      "\n",
      "============================== PL ==============================\n",
      "ðŸ”§ Generating real graphs from JSON files...\n",
      "  Processing training: 9662 entries for PL...\n",
      "  training PL: 9312 real graphs saved, 350 failed\n",
      "  Processing validation: 903 entries for PL...\n",
      "  validation PL: 871 real graphs saved, 32 failed\n",
      "  Processing core: 257 entries for PL...\n",
      "  core PL: 249 real graphs saved, 8 failed\n",
      "  Processing holdout: 3393 entries for PL...\n",
      "  holdout PL: 3232 real graphs saved, 161 failed\n",
      "ðŸ¤– Generating synthetic graphs using augmentation...\n",
      "  Generating synthetic graphs for PL using Controlled Augmentation...\n",
      "    Found 9312 PDB directories\n",
      "    Progress: 0/9312\n",
      "    Progress: 500/9312\n",
      "    Progress: 1000/9312\n",
      "    Progress: 1500/9312\n",
      "    Progress: 2000/9312\n",
      "    Progress: 2500/9312\n",
      "    Progress: 3000/9312\n",
      "    Progress: 3500/9312\n",
      "    Progress: 4000/9312\n",
      "    Progress: 4500/9312\n",
      "    Progress: 5000/9312\n",
      "    Progress: 5500/9312\n",
      "    Progress: 6000/9312\n",
      "    Progress: 6500/9312\n",
      "    Progress: 7000/9312\n",
      "    Progress: 7500/9312\n",
      "    Progress: 8000/9312\n",
      "    Progress: 8500/9312\n",
      "    Progress: 9000/9312\n",
      "  PL: 9312 synthetic graphs generated, 0 failed\n",
      "    Failure breakdown: {'no_graph': 0, 'no_affinity': 0, 'generation_failed': 0, 'empty_result': 0}\n",
      "  Generating synthetic graphs for PL using Controlled Augmentation...\n",
      "    Found 871 PDB directories\n",
      "    Progress: 0/871\n",
      "    Progress: 500/871\n",
      "  PL: 871 synthetic graphs generated, 0 failed\n",
      "    Failure breakdown: {'no_graph': 0, 'no_affinity': 0, 'generation_failed': 0, 'empty_result': 0}\n",
      "\n",
      "============================== PI ==============================\n",
      "ðŸ”§ Generating real graphs from JSON files...\n",
      "  Processing training: 9662 entries for PI...\n",
      "  training PI: 9312 real graphs saved, 350 failed\n",
      "  Processing validation: 903 entries for PI...\n",
      "  validation PI: 871 real graphs saved, 32 failed\n",
      "  Processing core: 257 entries for PI...\n",
      "  core PI: 249 real graphs saved, 8 failed\n",
      "  Processing holdout: 3393 entries for PI...\n",
      "  holdout PI: 3232 real graphs saved, 161 failed\n",
      "ðŸ¤– Generating synthetic graphs using augmentation...\n",
      "  Generating synthetic graphs for PI using Controlled Augmentation...\n",
      "    Found 9312 PDB directories\n",
      "    Progress: 0/9312\n",
      "    Progress: 500/9312\n",
      "    Progress: 1000/9312\n",
      "    Progress: 1500/9312\n",
      "    Progress: 2000/9312\n",
      "    Progress: 2500/9312\n",
      "    Progress: 3000/9312\n",
      "    Progress: 3500/9312\n",
      "    Progress: 4000/9312\n",
      "    Progress: 4500/9312\n",
      "    Progress: 5000/9312\n",
      "    Progress: 5500/9312\n",
      "    Progress: 6000/9312\n",
      "    Progress: 6500/9312\n",
      "    Progress: 7000/9312\n",
      "    Progress: 7500/9312\n",
      "    Progress: 8000/9312\n",
      "    Progress: 8500/9312\n",
      "    Progress: 9000/9312\n",
      "  PI: 9312 synthetic graphs generated, 0 failed\n",
      "    Failure breakdown: {'no_graph': 0, 'no_affinity': 0, 'generation_failed': 0, 'empty_result': 0}\n",
      "  Generating synthetic graphs for PI using Controlled Augmentation...\n",
      "    Found 871 PDB directories\n",
      "    Progress: 0/871\n",
      "    Progress: 500/871\n",
      "  PI: 871 synthetic graphs generated, 0 failed\n",
      "    Failure breakdown: {'no_graph': 0, 'no_affinity': 0, 'generation_failed': 0, 'empty_result': 0}\n",
      "\n",
      "============================== LI ==============================\n",
      "ðŸ”§ Generating real graphs from JSON files...\n",
      "  Processing training: 9662 entries for LI...\n",
      "  training LI: 9312 real graphs saved, 350 failed\n",
      "  Processing validation: 903 entries for LI...\n",
      "  validation LI: 871 real graphs saved, 32 failed\n",
      "  Processing core: 257 entries for LI...\n",
      "  core LI: 249 real graphs saved, 8 failed\n",
      "  Processing holdout: 3393 entries for LI...\n",
      "  holdout LI: 3232 real graphs saved, 161 failed\n",
      "ðŸ¤– Generating synthetic graphs using augmentation...\n",
      "  Generating synthetic graphs for LI using Controlled Augmentation...\n",
      "    Found 9312 PDB directories\n",
      "    Progress: 0/9312\n",
      "    Progress: 500/9312\n",
      "    Progress: 1000/9312\n",
      "    Progress: 1500/9312\n",
      "    Progress: 2000/9312\n",
      "    Progress: 2500/9312\n",
      "    Progress: 3000/9312\n",
      "    Progress: 3500/9312\n",
      "    Progress: 4000/9312\n",
      "    Progress: 4500/9312\n",
      "    Progress: 5000/9312\n",
      "    Progress: 5500/9312\n",
      "    Progress: 6000/9312\n",
      "    Progress: 6500/9312\n",
      "    Progress: 7000/9312\n",
      "    Progress: 7500/9312\n",
      "    Progress: 8000/9312\n",
      "    Progress: 8500/9312\n",
      "    Progress: 9000/9312\n",
      "  LI: 9312 synthetic graphs generated, 0 failed\n",
      "    Failure breakdown: {'no_graph': 0, 'no_affinity': 0, 'generation_failed': 0, 'empty_result': 0}\n",
      "  Generating synthetic graphs for LI using Controlled Augmentation...\n",
      "    Found 871 PDB directories\n",
      "    Progress: 0/871\n",
      "    Progress: 500/871\n",
      "  LI: 871 synthetic graphs generated, 0 failed\n",
      "    Failure breakdown: {'no_graph': 0, 'no_affinity': 0, 'generation_failed': 0, 'empty_result': 0}\n",
      "\n",
      "============================== PLI ==============================\n",
      "ðŸ”§ Generating real graphs from JSON files...\n",
      "  Processing training: 9662 entries for PLI...\n",
      "  training PLI: 9312 real graphs saved, 350 failed\n",
      "  Processing validation: 903 entries for PLI...\n",
      "  validation PLI: 871 real graphs saved, 32 failed\n",
      "  Processing core: 257 entries for PLI...\n",
      "  core PLI: 249 real graphs saved, 8 failed\n",
      "  Processing holdout: 3393 entries for PLI...\n",
      "  holdout PLI: 3232 real graphs saved, 161 failed\n",
      "ðŸ¤– Generating synthetic graphs using augmentation...\n",
      "  Generating synthetic graphs for PLI using Controlled Augmentation...\n",
      "    Found 9312 PDB directories\n",
      "    Progress: 0/9312\n",
      "    Progress: 500/9312\n",
      "    Progress: 1000/9312\n",
      "    Progress: 1500/9312\n",
      "    Progress: 2000/9312\n",
      "    Progress: 2500/9312\n",
      "    Progress: 3000/9312\n",
      "    Progress: 3500/9312\n",
      "    Progress: 4000/9312\n",
      "    Progress: 4500/9312\n",
      "    Progress: 5000/9312\n",
      "    Progress: 5500/9312\n",
      "    Progress: 6000/9312\n",
      "    Progress: 6500/9312\n",
      "    Progress: 7000/9312\n",
      "    Progress: 7500/9312\n",
      "    Progress: 8000/9312\n",
      "    Progress: 8500/9312\n",
      "    Progress: 9000/9312\n",
      "  PLI: 9312 synthetic graphs generated, 0 failed\n",
      "    Failure breakdown: {'no_graph': 0, 'no_affinity': 0, 'generation_failed': 0, 'empty_result': 0}\n",
      "  Generating synthetic graphs for PLI using Controlled Augmentation...\n",
      "    Found 871 PDB directories\n",
      "    Progress: 0/871\n",
      "    Progress: 500/871\n",
      "  PLI: 871 synthetic graphs generated, 0 failed\n",
      "    Failure breakdown: {'no_graph': 0, 'no_affinity': 0, 'generation_failed': 0, 'empty_result': 0}\n",
      "\n",
      "============================================================\n",
      "âœ… COMPLETE PIPELINE FINISHED!\n",
      "Total time: 48.5 minutes\n",
      "Output saved to: D:\\PhD\\Chapter_4\\Code2\\pdbbind\\complete_graphs_20250709_163209\n",
      "training: 9312 PDB directories\n",
      "validation: 871 PDB directories\n",
      "core: 249 PDB directories\n",
      "holdout: 3232 PDB directories\n",
      "training_synthetic: 9312 PDB directories\n",
      "validation_synthetic: 871 PDB directories\n",
      "\n",
      "ðŸ“ Directory structure created:\n",
      "  D:\\PhD\\Chapter_4\\Code2\\pdbbind\\complete_graphs_20250709_163209/\n",
      "    â”œâ”€â”€ training/\n",
      "    â”œâ”€â”€ validation/\n",
      "    â”œâ”€â”€ core/\n",
      "    â”œâ”€â”€ holdout/\n",
      "    â”œâ”€â”€ training_synthetic/\n",
      "    â””â”€â”€ validation_synthetic/\n",
      "\n",
      "Each PDB directory contains:\n",
      "  - {pdb_id}_{combination}.pkl (graph data)\n",
      "  - {pdb_id}_affinity.pkl (binding affinity)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool, global_add_pool\n",
    "from torch_geometric.utils import add_self_loops, to_undirected\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Atom property dictionary\n",
    "atom_property_dict = {\n",
    "    'H': {'atomic_num': 1, 'mass': 1.008, 'electronegativity': 2.20, 'vdw_radius': 1.20},\n",
    "    'C': {'atomic_num': 6, 'mass': 12.011, 'electronegativity': 2.55, 'vdw_radius': 1.70},\n",
    "    'N': {'atomic_num': 7, 'mass': 14.007, 'electronegativity': 3.04, 'vdw_radius': 1.55},\n",
    "    'O': {'atomic_num': 8, 'mass': 15.999, 'electronegativity': 3.44, 'vdw_radius': 1.52},\n",
    "    'P': {'atomic_num': 15, 'mass': 30.974, 'electronegativity': 2.19, 'vdw_radius': 1.80},\n",
    "    'S': {'atomic_num': 16, 'mass': 32.065, 'electronegativity': 2.58, 'vdw_radius': 1.80},\n",
    "    'F': {'atomic_num': 9, 'mass': 18.998, 'electronegativity': 3.98, 'vdw_radius': 1.47},\n",
    "    'Cl': {'atomic_num': 17, 'mass': 35.453, 'electronegativity': 3.16, 'vdw_radius': 1.75},\n",
    "    'Br': {'atomic_num': 35, 'mass': 79.904, 'electronegativity': 2.96, 'vdw_radius': 1.85},\n",
    "    'I': {'atomic_num': 53, 'mass': 126.904, 'electronegativity': 2.66, 'vdw_radius': 1.98},\n",
    "    'CA': {'atomic_num': 6, 'mass': 12.011, 'electronegativity': 2.55, 'vdw_radius': 1.70},\n",
    "    'CZ': {'atomic_num': 6, 'mass': 12.011, 'electronegativity': 2.55, 'vdw_radius': 1.70},\n",
    "    'OG': {'atomic_num': 8, 'mass': 15.999, 'electronegativity': 3.44, 'vdw_radius': 1.52},\n",
    "    'ZN': {'atomic_num': 30, 'mass': 65.38, 'electronegativity': 1.65, 'vdw_radius': 1.39},\n",
    "    'MG': {'atomic_num': 12, 'mass': 24.305, 'electronegativity': 1.31, 'vdw_radius': 1.73},\n",
    "    'FE': {'atomic_num': 26, 'mass': 55.845, 'electronegativity': 1.83, 'vdw_radius': 1.72},\n",
    "    'MN': {'atomic_num': 25, 'mass': 54.938, 'electronegativity': 1.55, 'vdw_radius': 1.73},\n",
    "    'CU': {'atomic_num': 29, 'mass': 63.546, 'electronegativity': 1.90, 'vdw_radius': 1.40},\n",
    "}\n",
    "\n",
    "def load_csv(csv_path, use_half=False):\n",
    "    \"\"\"Load CSV with optional half sampling\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df[df['Affinity_pK'] != 0]\n",
    "    \n",
    "    if use_half:\n",
    "        half_size = len(df) // 2\n",
    "        df = df.head(half_size)\n",
    "        print(f\"Using half dataset: {half_size} samples from {csv_path}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_enhanced_features(node, atom_property_dict, graph_type='P'):\n",
    "    \"\"\"Create enhanced node features\"\"\"\n",
    "    atom_type = node['attype']\n",
    "    prop = atom_property_dict.get(atom_type, \n",
    "                                 {'atomic_num': 6, 'mass': 12.011, 'electronegativity': 2.55, 'vdw_radius': 1.70})\n",
    "    \n",
    "    if 'pl' in node:\n",
    "        is_protein = node['pl'] == 'P'\n",
    "        is_ligand = node['pl'] == 'L'\n",
    "        is_interaction = graph_type == 'I'\n",
    "    else:\n",
    "        is_protein = graph_type == 'P'\n",
    "        is_ligand = graph_type == 'L'\n",
    "        is_interaction = graph_type == 'I'\n",
    "    \n",
    "    features = [\n",
    "        prop['atomic_num'] / 30.0, prop['mass'] / 100.0, prop['electronegativity'] / 4.0, prop['vdw_radius'] / 2.0,\n",
    "        prop['atomic_num'] ** 0.5 / 5.5, prop['mass'] / prop['atomic_num'], 1.0 / prop['electronegativity'], prop['vdw_radius'] ** 2,\n",
    "        1.0 if prop['atomic_num'] in [6] else 0.0, 1.0 if prop['atomic_num'] in [7] else 0.0,\n",
    "        1.0 if prop['atomic_num'] in [8] else 0.0, 1.0 if prop['atomic_num'] in [16] else 0.0,\n",
    "        1.0 if prop['atomic_num'] > 10 else 0.0, 1.0 if prop['electronegativity'] > 3.0 else 0.0,\n",
    "        1.0 if is_protein else 0.0, 1.0 if is_ligand else 0.0, 1.0 if is_interaction else 0.0,\n",
    "    ]\n",
    "    return features\n",
    "\n",
    "def load_single_graph(pdb_id, base_path, graph_type):\n",
    "    \"\"\"Load single graph with proper node types\"\"\"\n",
    "    if graph_type == 'P':\n",
    "        json_path = os.path.join(base_path, pdb_id, f'{pdb_id}_protein_graph.json')\n",
    "    elif graph_type == 'L':\n",
    "        json_path = os.path.join(base_path, pdb_id, f'{pdb_id}_ligand_graph.json')\n",
    "    elif graph_type == 'I':\n",
    "        json_path = os.path.join(base_path, pdb_id, f'{pdb_id}_interaction_graph.json')\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        with open(json_path, 'r') as file:\n",
    "            graph = json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        return None\n",
    "\n",
    "    if not graph['nodes']:\n",
    "        return None\n",
    "\n",
    "    node_features = []\n",
    "    node_types = []\n",
    "    \n",
    "    for node in graph['nodes']:\n",
    "        features = create_enhanced_features(node, atom_property_dict, graph_type)\n",
    "        node_features.append(features)\n",
    "        \n",
    "        # Extract proper node types\n",
    "        if 'pl' in node:\n",
    "            node_types.append(node['pl'])\n",
    "        else:\n",
    "            node_types.append(graph_type)\n",
    "\n",
    "    node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "    \n",
    "    edge_index = []\n",
    "    edge_features = []\n",
    "    \n",
    "    for edge in graph['edges']:\n",
    "        if edge['id1'] is not None and edge['id2'] is not None:\n",
    "            length = max(edge['length'], 0.1)\n",
    "            edge_index.append([edge['id1'], edge['id2']])\n",
    "            edge_features.append([length / 10.0, 1.0 / length, np.exp(-length/2.0)])\n",
    "\n",
    "    if not edge_index:\n",
    "        num_nodes = len(node_features)\n",
    "        edge_index = torch.arange(num_nodes).unsqueeze(0).repeat(2, 1)\n",
    "        edge_features = torch.ones(num_nodes, 3) * 0.5\n",
    "    else:\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "        edge_features = torch.tensor(edge_features, dtype=torch.float)\n",
    "        edge_index = to_undirected(edge_index)\n",
    "        if edge_features.size(0) * 2 == edge_index.size(1):\n",
    "            edge_features = edge_features.repeat(2, 1)\n",
    "\n",
    "    return {\n",
    "        'node_features': node_features,\n",
    "        'edge_index': edge_index,\n",
    "        'edge_features': edge_features,\n",
    "        'num_nodes': len(node_features),\n",
    "        'graph_type': graph_type,\n",
    "        'node_types': node_types,\n",
    "        'pdb_id': pdb_id,\n",
    "        'combination': graph_type\n",
    "    }\n",
    "\n",
    "def merge_graphs(graphs):\n",
    "    \"\"\"Merge multiple graphs preserving node types\"\"\"\n",
    "    all_node_features = []\n",
    "    all_edge_indices = []\n",
    "    all_edge_features = []\n",
    "    all_node_types = []\n",
    "    \n",
    "    node_offset = 0\n",
    "    \n",
    "    for graph in graphs:\n",
    "        if graph is None:\n",
    "            continue\n",
    "            \n",
    "        all_node_features.append(graph['node_features'])\n",
    "        adjusted_edge_index = graph['edge_index'] + node_offset\n",
    "        all_edge_indices.append(adjusted_edge_index)\n",
    "        all_edge_features.append(graph['edge_features'])\n",
    "        all_node_types.extend(graph['node_types'])\n",
    "        \n",
    "        node_offset += graph['num_nodes']\n",
    "    \n",
    "    if not all_node_features:\n",
    "        return None\n",
    "    \n",
    "    merged_node_features = torch.cat(all_node_features, dim=0)\n",
    "    merged_edge_index = torch.cat(all_edge_indices, dim=1) if all_edge_indices else torch.empty((2, 0), dtype=torch.long)\n",
    "    merged_edge_features = torch.cat(all_edge_features, dim=0) if all_edge_features else torch.empty((0, 3))\n",
    "    \n",
    "    return {\n",
    "        'node_features': merged_node_features,\n",
    "        'edge_index': merged_edge_index,\n",
    "        'edge_features': merged_edge_features,\n",
    "        'node_types': all_node_types,\n",
    "        'num_nodes': merged_node_features.size(0)\n",
    "    }\n",
    "\n",
    "def load_combined_graph(pdb_id, base_path, combination):\n",
    "    \"\"\"Load and combine graphs for specific combination\"\"\"\n",
    "    graphs_to_load = []\n",
    "    \n",
    "    if 'P' in combination:\n",
    "        graphs_to_load.append('P')\n",
    "    if 'L' in combination:\n",
    "        graphs_to_load.append('L')\n",
    "    if 'I' in combination:\n",
    "        graphs_to_load.append('I')\n",
    "    \n",
    "    loaded_graphs = []\n",
    "    for graph_type in graphs_to_load:\n",
    "        graph = load_single_graph(pdb_id, base_path, graph_type)\n",
    "        loaded_graphs.append(graph)\n",
    "    \n",
    "    merged_result = merge_graphs(loaded_graphs)\n",
    "    if merged_result is None:\n",
    "        return None\n",
    "    \n",
    "    # Store complete graph data\n",
    "    graph_data = {\n",
    "        'pdb_id': pdb_id,\n",
    "        'combination': combination,\n",
    "        'node_features': merged_result['node_features'].numpy().tolist(),\n",
    "        'edge_index': merged_result['edge_index'].numpy().tolist(),\n",
    "        'edge_features': merged_result['edge_features'].numpy().tolist(),\n",
    "        'node_types': merged_result['node_types'],\n",
    "        'num_nodes': merged_result['num_nodes']\n",
    "    }\n",
    "    \n",
    "    return graph_data\n",
    "\n",
    "def save_graph_data(graph_data, output_dir, dataset_name, affinity):\n",
    "    \"\"\"Save graph data with affinity\"\"\"\n",
    "    pdb_id = graph_data['pdb_id']\n",
    "    combination = graph_data['combination']\n",
    "    \n",
    "    # Create directory\n",
    "    pdb_dir = os.path.join(output_dir, dataset_name, pdb_id)\n",
    "    os.makedirs(pdb_dir, exist_ok=True)\n",
    "    \n",
    "    # Save graph\n",
    "    graph_path = os.path.join(pdb_dir, f'{pdb_id}_{combination}.pkl')\n",
    "    with open(graph_path, 'wb') as f:\n",
    "        pickle.dump(graph_data, f)\n",
    "    \n",
    "    # Save affinity\n",
    "    affinity_path = os.path.join(pdb_dir, f'{pdb_id}_affinity.pkl')\n",
    "    affinity_data = {'affinity': affinity, 'pdb_id': pdb_id}\n",
    "    with open(affinity_path, 'wb') as f:\n",
    "        pickle.dump(affinity_data, f)\n",
    "\n",
    "# Fast GAN Classes\n",
    "class FastGraphAugmentationGenerator(nn.Module):\n",
    "    def __init__(self, node_features=17, edge_features=3, hidden_dim=64):\n",
    "        super(ImprovedFastGraphAugmentationGenerator, self).__init__()\n",
    "        \n",
    "        self.node_encoder = nn.Sequential(\n",
    "            nn.Linear(node_features, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 32)\n",
    "        )\n",
    "        \n",
    "        # Less aggressive dropout (max 15% instead of 30%)\n",
    "        self.node_drop_predictor = nn.Sequential(\n",
    "            nn.Linear(32, 1), \n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.edge_drop_predictor = nn.Sequential(\n",
    "            nn.Linear(64, 1), \n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Smaller noise scale\n",
    "        self.node_noise_predictor = nn.Sequential(\n",
    "            nn.Linear(32, node_features), \n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.edge_noise_predictor = nn.Sequential(\n",
    "            nn.Linear(64, edge_features), \n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, node_features, edge_index, edge_features):\n",
    "        node_emb = self.node_encoder(node_features)\n",
    "        node_drop_probs = self.node_drop_predictor(node_emb).squeeze()\n",
    "        node_noise = self.node_noise_predictor(node_emb) * 0.08  # Reduced from 0.15\n",
    "        \n",
    "        edge_embs = []\n",
    "        for i in range(edge_index.size(1)):\n",
    "            edge_emb = torch.cat([node_emb[edge_index[0, i]], node_emb[edge_index[1, i]]])\n",
    "            edge_embs.append(edge_emb)\n",
    "        \n",
    "        if edge_embs:\n",
    "            edge_embs = torch.stack(edge_embs)\n",
    "            edge_drop_probs = self.edge_drop_predictor(edge_embs).squeeze()\n",
    "            edge_noise = self.edge_noise_predictor(edge_embs) * 0.06  # Reduced from 0.12\n",
    "        else:\n",
    "            edge_drop_probs = torch.tensor([])\n",
    "            edge_noise = torch.empty(0, edge_features.size(-1))\n",
    "        \n",
    "        return node_drop_probs, edge_drop_probs, node_noise, edge_noise\n",
    "\n",
    "class FastGraphDiscriminator(nn.Module):\n",
    "    def __init__(self, node_features=17, edge_features=3, hidden_dim=64):\n",
    "        super(FastGraphDiscriminator, self).__init__()\n",
    "        \n",
    "        self.conv1 = GCNConv(node_features, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, 32)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(32, 64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        return self.classifier(x)\n",
    "\n",
    "def prepare_real_graphs(df, base_path, combination, output_dir, dataset_name):\n",
    "    \"\"\"Prepare real graphs from JSON files\"\"\"\n",
    "    print(f\"  Processing {dataset_name}: {len(df)} entries for {combination}...\")\n",
    "    \n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        pdb_id, affinity = row['PDB_ID'], row['Affinity_pK']\n",
    "        \n",
    "        if np.isnan(affinity) or np.isinf(affinity):\n",
    "            failed += 1\n",
    "            continue\n",
    "            \n",
    "        graph_data = load_combined_graph(pdb_id, base_path, combination)\n",
    "        if graph_data is not None:\n",
    "            save_graph_data(graph_data, output_dir, dataset_name, affinity)\n",
    "            successful += 1\n",
    "        else:\n",
    "            failed += 1\n",
    "    \n",
    "    print(f\"  {dataset_name} {combination}: {successful} real graphs saved, {failed} failed\")\n",
    "    return successful\n",
    "\n",
    "# GAN Components for Graph Generation\n",
    "class GraphEncoder(nn.Module):\n",
    "    def __init__(self, node_features=17, edge_features=3, hidden_dim=128, latent_dim=64):\n",
    "        super(GraphEncoder, self).__init__()\n",
    "        self.node_norm = nn.LayerNorm(node_features)\n",
    "        self.edge_norm = nn.LayerNorm(edge_features)\n",
    "        \n",
    "        self.node_conv1 = GCNConv(node_features, hidden_dim)\n",
    "        self.node_conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.node_conv3 = GCNConv(hidden_dim, latent_dim)\n",
    "        \n",
    "        self.node_bn1 = nn.LayerNorm(hidden_dim)\n",
    "        self.node_bn2 = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(edge_features, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, latent_dim)\n",
    "        )\n",
    "        \n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 32),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "        \n",
    "        self.graph_mlp = nn.Sequential(\n",
    "            nn.Linear(latent_dim * 3, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, latent_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        x = self.node_norm(x)\n",
    "        \n",
    "        h1 = F.relu(self.node_bn1(self.node_conv1(x, edge_index)))\n",
    "        h2 = F.relu(self.node_bn2(self.node_conv2(h1, edge_index)))\n",
    "        node_emb = self.node_conv3(h2, edge_index)\n",
    "        \n",
    "        node_pool_mean = global_mean_pool(node_emb, batch)\n",
    "        \n",
    "        att_weights = self.attention(node_emb)\n",
    "        att_weights = F.softmax(att_weights, dim=0)\n",
    "        node_pool_att = global_add_pool(node_emb * att_weights, batch)\n",
    "        \n",
    "        if edge_attr.size(0) > 0:\n",
    "            edge_attr = self.edge_norm(edge_attr)\n",
    "            edge_emb = self.edge_mlp(edge_attr)\n",
    "            edge_batch = batch[edge_index[0]]\n",
    "            edge_pool = global_mean_pool(edge_emb, edge_batch)\n",
    "            \n",
    "            num_graphs = batch.max().item() + 1\n",
    "            if edge_pool.size(0) < num_graphs:\n",
    "                padding = torch.zeros(num_graphs - edge_pool.size(0), edge_pool.size(1)).to(x.device)\n",
    "                edge_pool = torch.cat([edge_pool, padding], dim=0)\n",
    "        else:\n",
    "            num_graphs = batch.max().item() + 1\n",
    "            edge_pool = torch.zeros(num_graphs, self.edge_mlp[-1].out_features).to(x.device)\n",
    "        \n",
    "        graph_emb = torch.cat([node_pool_mean, node_pool_att, edge_pool], dim=1)\n",
    "        return self.graph_mlp(graph_emb)\n",
    "\n",
    "class GraphAugmentationGenerator(nn.Module):\n",
    "    def __init__(self, node_features=17, edge_features=3, hidden_dim=128):\n",
    "        super(GraphAugmentationGenerator, self).__init__()\n",
    "        \n",
    "        # Node feature augmentation\n",
    "        self.node_augment = nn.Sequential(\n",
    "            nn.Linear(node_features, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, node_features),\n",
    "            nn.Tanh()  # Scale augmentation\n",
    "        )\n",
    "        \n",
    "        # Edge augmentation\n",
    "        self.edge_augment = nn.Sequential(\n",
    "            nn.Linear(edge_features, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, edge_features),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        # Edge probability modification\n",
    "        self.edge_prob_mlp = nn.Sequential(\n",
    "            nn.Linear(node_features * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, node_features, edge_index, edge_features):\n",
    "        # Augment node features\n",
    "        node_noise = self.node_augment(node_features) * 0.15  # 15% max change\n",
    "        augmented_nodes = node_features + node_noise\n",
    "        \n",
    "        # Augment edge features\n",
    "        if edge_features.size(0) > 0:\n",
    "            edge_noise = self.edge_augment(edge_features) * 0.12  # 12% max change\n",
    "            augmented_edges = edge_features + edge_noise\n",
    "        else:\n",
    "            augmented_edges = edge_features\n",
    "        \n",
    "        # Edge probability modifications\n",
    "        edge_probs = []\n",
    "        for i in range(edge_index.size(1)):\n",
    "            node_i = augmented_nodes[edge_index[0, i]]\n",
    "            node_j = augmented_nodes[edge_index[1, i]]\n",
    "            edge_input = torch.cat([node_i, node_j])\n",
    "            prob = self.edge_prob_mlp(edge_input)\n",
    "            edge_probs.append(prob)\n",
    "        \n",
    "        edge_probs = torch.stack(edge_probs).squeeze()\n",
    "        \n",
    "        return augmented_nodes, augmented_edges, edge_probs\n",
    "\n",
    "class GraphDiscriminator(nn.Module):\n",
    "    def __init__(self, node_features=17, edge_features=3, hidden_dim=128):\n",
    "        super(GraphDiscriminator, self).__init__()\n",
    "        \n",
    "        self.encoder = GraphEncoder(node_features, edge_features, hidden_dim, 64)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.Linear(64, 128)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.utils.spectral_norm(nn.Linear(128, 64)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.utils.spectral_norm(nn.Linear(64, 1))\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        graph_emb = self.encoder(x, edge_index, edge_attr, batch)\n",
    "        return self.classifier(graph_emb)\n",
    "\n",
    "class MolecularGraphGAN:\n",
    "    def __init__(self, device='cuda'):\n",
    "        self.device = device\n",
    "        self.generator = GraphAugmentationGenerator().to(device)\n",
    "        self.discriminator = GraphDiscriminator().to(device)\n",
    "        \n",
    "        self.g_optimizer = torch.optim.Adam(self.generator.parameters(), lr=0.001)\n",
    "        self.d_optimizer = torch.optim.Adam(self.discriminator.parameters(), lr=0.002)\n",
    "        \n",
    "    def convert_graph_to_pyg(self, graph_data):\n",
    "        \"\"\"Convert graph dict to PyG Data object\"\"\"\n",
    "        node_features = torch.tensor(graph_data['node_features'], dtype=torch.float)\n",
    "        edge_index = torch.tensor(graph_data['edge_index'], dtype=torch.long)\n",
    "        edge_attr = torch.tensor(graph_data['edge_features'], dtype=torch.float)\n",
    "        \n",
    "        if edge_index.size(0) != 2:\n",
    "            edge_index = edge_index.t()\n",
    "        \n",
    "        return Data(x=node_features, edge_index=edge_index, edge_attr=edge_attr)\n",
    "        \n",
    "    def generate_synthetic_graph(self, original_graph, pdb_id, combination):\n",
    "        \"\"\"Generate synthetic graph by augmenting original\"\"\"\n",
    "        try:\n",
    "            # Convert to PyG format\n",
    "            pyg_data = self.convert_graph_to_pyg(original_graph)\n",
    "            pyg_data = pyg_data.to(self.device)\n",
    "            \n",
    "            # Train GAN on this specific graph\n",
    "            self.train_on_graph(pyg_data, epochs=30)\n",
    "            \n",
    "            # Generate augmented version\n",
    "            self.generator.eval()\n",
    "            with torch.no_grad():\n",
    "                aug_nodes, aug_edges, edge_probs = self.generator(\n",
    "                    pyg_data.x, pyg_data.edge_index, pyg_data.edge_attr\n",
    "                )\n",
    "                \n",
    "                # Apply edge modifications based on probabilities\n",
    "                edge_mask = edge_probs > 0.7  # Keep edges with high probability\n",
    "                kept_edges = pyg_data.edge_index[:, edge_mask]\n",
    "                kept_edge_attrs = aug_edges[edge_mask] if aug_edges.size(0) > 0 else aug_edges\n",
    "                \n",
    "                # Create synthetic graph\n",
    "                synthetic_graph = deepcopy(original_graph)\n",
    "                synthetic_graph['node_features'] = aug_nodes.cpu().numpy().tolist()\n",
    "                synthetic_graph['edge_index'] = kept_edges.cpu().numpy().tolist()\n",
    "                synthetic_graph['edge_features'] = kept_edge_attrs.cpu().numpy().tolist()\n",
    "                synthetic_graph['node_types'] = original_graph['node_types'].copy()\n",
    "                synthetic_graph['pdb_id'] = f\"{pdb_id}_synthetic\"\n",
    "                synthetic_graph['synthetic_method'] = 'GAN_Augmentation'\n",
    "                synthetic_graph['combination'] = combination\n",
    "                \n",
    "                return synthetic_graph\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"    GAN error for {pdb_id}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def train_on_graph(self, pyg_data, epochs=30):\n",
    "        \"\"\"Train GAN using original graph as target\"\"\"\n",
    "        batch = torch.zeros(pyg_data.x.size(0), dtype=torch.long, device=self.device)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Train Discriminator\n",
    "            self.d_optimizer.zero_grad()\n",
    "            \n",
    "            # Real graph score\n",
    "            real_score = self.discriminator(pyg_data.x, pyg_data.edge_index, pyg_data.edge_attr, batch)\n",
    "            \n",
    "            # Generate augmented version\n",
    "            aug_nodes, aug_edges, edge_probs = self.generator(pyg_data.x, pyg_data.edge_index, pyg_data.edge_attr)\n",
    "            \n",
    "            # Apply edge sampling\n",
    "            edge_mask = edge_probs > 0.7\n",
    "            if edge_mask.sum() > 0:\n",
    "                fake_edges = pyg_data.edge_index[:, edge_mask]\n",
    "                fake_edge_attrs = aug_edges[edge_mask] if aug_edges.size(0) > 0 else aug_edges\n",
    "                fake_batch = torch.zeros(aug_nodes.size(0), dtype=torch.long, device=self.device)\n",
    "                fake_score = self.discriminator(aug_nodes, fake_edges, fake_edge_attrs, fake_batch)\n",
    "                \n",
    "                # WGAN loss\n",
    "                d_loss = fake_score.mean() - real_score.mean()\n",
    "                d_loss.backward()\n",
    "                self.d_optimizer.step()\n",
    "            \n",
    "            # Train Generator (every 2 epochs)\n",
    "            if epoch % 2 == 0:\n",
    "                self.g_optimizer.zero_grad()\n",
    "                aug_nodes, aug_edges, edge_probs = self.generator(pyg_data.x, pyg_data.edge_index, pyg_data.edge_attr)\n",
    "                \n",
    "                edge_mask = edge_probs > 0.7\n",
    "                if edge_mask.sum() > 0:\n",
    "                    fake_edges = pyg_data.edge_index[:, edge_mask]\n",
    "                    fake_edge_attrs = aug_edges[edge_mask] if aug_edges.size(0) > 0 else aug_edges\n",
    "                    fake_batch = torch.zeros(aug_nodes.size(0), dtype=torch.long, device=self.device)\n",
    "                    fake_score = self.discriminator(aug_nodes, fake_edges, fake_edge_attrs, fake_batch)\n",
    "                    \n",
    "                    g_loss = -fake_score.mean()\n",
    "                    g_loss.backward()\n",
    "                    self.g_optimizer.step()\n",
    "\n",
    "class StructureAwareGAN:\n",
    "    def __init__(self, device='cuda'):\n",
    "        self.device = device\n",
    "        self.gan = MolecularGraphGAN(device=device)\n",
    "        \n",
    "    def generate_synthetic_graph(self, original_graph, pdb_id, combination):\n",
    "        return self.gan.generate_synthetic_graph(original_graph, pdb_id, combination)\n",
    "        \n",
    "    def augment_molecular_graph(self, original_graph, pdb_id, combination):\n",
    "        \"\"\"Create synthetic graph by controlled augmentation\"\"\"\n",
    "        try:\n",
    "            synthetic_graph = deepcopy(original_graph)\n",
    "            \n",
    "            node_features = np.array(original_graph['node_features'])\n",
    "            edge_index = original_graph['edge_index']\n",
    "            edge_features = np.array(original_graph['edge_features']) if original_graph.get('edge_features') else None\n",
    "            \n",
    "            n_nodes = len(node_features)\n",
    "            n_edges = len(edge_index[0]) if edge_index and len(edge_index) >= 2 else 0\n",
    "            \n",
    "            # print(f\"    DEBUG {pdb_id}: Original - {n_nodes} nodes, {n_edges} edges\")\n",
    "            \n",
    "            # Skip augmentation if no edges\n",
    "            if n_edges == 0:\n",
    "                print(f\"    Warning: {pdb_id} has no edges, skipping augmentation\")\n",
    "                return None\n",
    "            \n",
    "            # 1. Add controlled noise to node features (10-15%)\n",
    "            noise_scale = random.uniform(0.10, 0.15)\n",
    "            node_noise = np.random.normal(0, noise_scale, node_features.shape)\n",
    "            augmented_nodes = node_features + node_noise\n",
    "            \n",
    "            # 2. Randomly remove some edges (15-25%) - FIXED\n",
    "            if n_edges > 3:\n",
    "                removal_rate = random.uniform(0.15, 0.25)\n",
    "                edges_to_remove = int(n_edges * removal_rate)\n",
    "                edges_to_keep = n_edges - edges_to_remove\n",
    "                \n",
    "                edge_indices = list(range(n_edges))\n",
    "                random.shuffle(edge_indices)\n",
    "                keep_indices = edge_indices[:edges_to_keep]\n",
    "                \n",
    "                new_edge_index = [\n",
    "                    [edge_index[0][i] for i in keep_indices],\n",
    "                    [edge_index[1][i] for i in keep_indices]\n",
    "                ]\n",
    "                \n",
    "                if edge_features is not None and len(edge_features) > 0:\n",
    "                    new_edge_features = edge_features[keep_indices]\n",
    "                else:\n",
    "                    new_edge_features = edge_features\n",
    "            else:\n",
    "                new_edge_index = deepcopy(edge_index)  # Use deepcopy\n",
    "                new_edge_features = deepcopy(edge_features) if edge_features is not None else None\n",
    "            \n",
    "            # 3. Add some new random edges (10-15%)\n",
    "            if n_nodes > 2:\n",
    "                addition_rate = random.uniform(0.10, 0.15)\n",
    "                edges_to_add = max(1, int(n_edges * addition_rate))\n",
    "                \n",
    "                for _ in range(edges_to_add):\n",
    "                    attempts = 0\n",
    "                    while attempts < 10:\n",
    "                        u, v = random.sample(range(n_nodes), 2)\n",
    "                        edge_exists = False\n",
    "                        for i in range(len(new_edge_index[0])):\n",
    "                            if (new_edge_index[0][i] == u and new_edge_index[1][i] == v) or \\\n",
    "                               (new_edge_index[0][i] == v and new_edge_index[1][i] == u):\n",
    "                                edge_exists = True\n",
    "                                break\n",
    "                        \n",
    "                        if not edge_exists:\n",
    "                            new_edge_index[0].append(u)\n",
    "                            new_edge_index[1].append(v)\n",
    "                            \n",
    "                            if new_edge_features is not None and len(new_edge_features) > 0:\n",
    "                                avg_edge_feat = np.mean(new_edge_features, axis=0)\n",
    "                                edge_noise = np.random.normal(0, 0.1, avg_edge_feat.shape)\n",
    "                                new_edge_feat = avg_edge_feat + edge_noise\n",
    "                                new_edge_features = np.vstack([new_edge_features, new_edge_feat])\n",
    "                            break\n",
    "                        attempts += 1\n",
    "            \n",
    "            # 4. Add noise to edge features (8-12%)\n",
    "            if new_edge_features is not None and len(new_edge_features) > 0:\n",
    "                edge_noise_scale = random.uniform(0.08, 0.12)\n",
    "                edge_noise = np.random.normal(0, edge_noise_scale, new_edge_features.shape)\n",
    "                new_edge_features = new_edge_features + edge_noise\n",
    "            \n",
    "            # Verify new edge structure\n",
    "            new_n_edges = len(new_edge_index[0]) if new_edge_index else 0\n",
    "            # print(f\"    DEBUG {pdb_id}: Synthetic - {n_nodes} nodes, {new_n_edges} edges\")\n",
    "            \n",
    "            if new_n_edges == 0:\n",
    "                print(f\"    Error: {pdb_id} synthetic graph has no edges after augmentation\")\n",
    "                return None\n",
    "            \n",
    "            # Update synthetic graph - PRESERVE NODE TYPES\n",
    "            synthetic_graph['node_features'] = augmented_nodes.tolist()\n",
    "            synthetic_graph['edge_index'] = new_edge_index\n",
    "            synthetic_graph['edge_features'] = new_edge_features.tolist() if new_edge_features is not None else []\n",
    "            \n",
    "            # CRITICAL: Preserve original node types exactly\n",
    "            synthetic_graph['node_types'] = original_graph['node_types'].copy()\n",
    "            \n",
    "            # Update metadata\n",
    "            synthetic_graph['pdb_id'] = f\"{pdb_id}_synthetic\"\n",
    "            synthetic_graph['synthetic_method'] = 'Controlled_Augmentation'\n",
    "            synthetic_graph['combination'] = combination\n",
    "            \n",
    "            return synthetic_graph\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    Augmentation error for {pdb_id}: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "class FastMolecularGraphGAN:\n",
    "    def __init__(self, device='cuda'):\n",
    "        self.device = device\n",
    "        self.generator = ImprovedFastGraphAugmentationGenerator().to(device)\n",
    "        self.discriminator = FastGraphDiscriminator().to(device)  # Keep original discriminator\n",
    "        self.g_optimizer = torch.optim.Adam(self.generator.parameters(), lr=0.002)\n",
    "        self.d_optimizer = torch.optim.Adam(self.discriminator.parameters(), lr=0.001)\n",
    "        \n",
    "    def convert_graph_to_pyg(self, graph_data):\n",
    "        node_features = torch.tensor(graph_data['node_features'], dtype=torch.float)\n",
    "        edge_index = torch.tensor(graph_data['edge_index'], dtype=torch.long)\n",
    "        edge_attr = torch.tensor(graph_data['edge_features'], dtype=torch.float)\n",
    "        if edge_index.size(0) != 2:\n",
    "            edge_index = edge_index.t()\n",
    "        return Data(x=node_features, edge_index=edge_index, edge_attr=edge_attr)\n",
    "    \n",
    "    def apply_conservative_augmentation(self, node_features, edge_index, edge_features, \n",
    "                                      node_drop_probs, edge_drop_probs, node_noise, edge_noise):\n",
    "        \"\"\"More conservative augmentation that preserves graph structure\"\"\"\n",
    "        \n",
    "        # Much less aggressive node dropping (max 10% instead of 30%)\n",
    "        node_drop_threshold = 0.1  # Reduced from 0.3\n",
    "        node_keep_mask = torch.rand(node_features.size(0), device=self.device) > (node_drop_probs * node_drop_threshold)\n",
    "        \n",
    "        # Ensure we keep at least 70% of nodes\n",
    "        min_nodes_to_keep = max(1, int(node_features.size(0) * 0.7))\n",
    "        if node_keep_mask.sum() < min_nodes_to_keep:\n",
    "            # Keep top nodes by inverse drop probability\n",
    "            _, top_indices = torch.topk(1 - node_drop_probs, min_nodes_to_keep)\n",
    "            node_keep_mask = torch.zeros_like(node_keep_mask, dtype=torch.bool)\n",
    "            node_keep_mask[top_indices] = True\n",
    "        \n",
    "        # Apply node augmentation\n",
    "        kept_nodes = node_features[node_keep_mask] + node_noise[node_keep_mask]\n",
    "        \n",
    "        # Create mapping for kept nodes\n",
    "        old_to_new = {old_idx: new_idx for new_idx, old_idx in enumerate(torch.where(node_keep_mask)[0].tolist())}\n",
    "        \n",
    "        # Process edges more conservatively\n",
    "        valid_edges = []\n",
    "        valid_edge_features = []\n",
    "        \n",
    "        edge_drop_threshold = 0.1  # Reduced from 0.3\n",
    "        \n",
    "        for i in range(edge_index.size(1)):\n",
    "            src, dst = edge_index[0, i].item(), edge_index[1, i].item()\n",
    "            \n",
    "            # Check if both nodes are kept\n",
    "            if src in old_to_new and dst in old_to_new:\n",
    "                # Less aggressive edge dropping\n",
    "                if len(edge_drop_probs) > i:\n",
    "                    keep_prob = 1.0 - edge_drop_probs[i] * edge_drop_threshold\n",
    "                else:\n",
    "                    keep_prob = 0.9  # Default keep probability\n",
    "                \n",
    "                if torch.rand(1).item() < keep_prob:\n",
    "                    valid_edges.append([old_to_new[src], old_to_new[dst]])\n",
    "                    if edge_features.size(0) > i:\n",
    "                        valid_edge_features.append(edge_features[i] + edge_noise[i])\n",
    "                    else:\n",
    "                        valid_edge_features.append(torch.zeros(edge_features.size(1), device=self.device))\n",
    "        \n",
    "        # Ensure we have some edges\n",
    "        if not valid_edges and kept_nodes.size(0) > 1:\n",
    "            # Create at least one edge between random nodes\n",
    "            n_nodes = kept_nodes.size(0)\n",
    "            for _ in range(min(3, n_nodes-1)):  # Add up to 3 random edges\n",
    "                u, v = random.sample(range(n_nodes), 2)\n",
    "                valid_edges.append([u, v])\n",
    "                if edge_features.size(1) > 0:\n",
    "                    avg_edge_feat = torch.mean(edge_features, dim=0) if edge_features.size(0) > 0 else torch.zeros(edge_features.size(1), device=self.device)\n",
    "                    valid_edge_features.append(avg_edge_feat)\n",
    "        \n",
    "        if valid_edges:\n",
    "            new_edge_index = torch.tensor(valid_edges, device=self.device).t()\n",
    "            new_edge_features = torch.stack(valid_edge_features)\n",
    "        else:\n",
    "            # Fallback: create minimal connectivity\n",
    "            n_nodes = kept_nodes.size(0)\n",
    "            if n_nodes > 1:\n",
    "                new_edge_index = torch.tensor([[0], [1]], device=self.device)\n",
    "                new_edge_features = torch.zeros(1, edge_features.size(1), device=self.device)\n",
    "            else:\n",
    "                new_edge_index = torch.tensor([[], []], device=self.device, dtype=torch.long)\n",
    "                new_edge_features = torch.zeros(0, edge_features.size(1), device=self.device)\n",
    "        \n",
    "        return kept_nodes, new_edge_index, new_edge_features, node_keep_mask\n",
    "    \n",
    "    def generate_synthetic_graph(self, original_graph, pdb_id, combination):\n",
    "        \"\"\"Generate synthetic graph with better preservation of structure\"\"\"\n",
    "        try:\n",
    "            pyg_data = self.convert_graph_to_pyg(original_graph).to(self.device)\n",
    "            \n",
    "            # Skip if graph is too small\n",
    "            if pyg_data.x.size(0) < 3:\n",
    "                return None\n",
    "                \n",
    "            self.generator.eval()\n",
    "            with torch.no_grad():\n",
    "                node_drop_probs, edge_drop_probs, node_noise, edge_noise = self.generator(\n",
    "                    pyg_data.x, pyg_data.edge_index, pyg_data.edge_attr\n",
    "                )\n",
    "                \n",
    "                aug_nodes, aug_edges, aug_edge_attrs, node_keep_mask = self.apply_conservative_augmentation(\n",
    "                    pyg_data.x, pyg_data.edge_index, pyg_data.edge_attr, \n",
    "                    node_drop_probs, edge_drop_probs, node_noise, edge_noise\n",
    "                )\n",
    "            \n",
    "            # Check if we have a valid graph\n",
    "            if aug_nodes.size(0) == 0:\n",
    "                print(f\"    Warning: {pdb_id} - No nodes after augmentation\")\n",
    "                return None\n",
    "                \n",
    "            # Map node types correctly\n",
    "            original_node_types = original_graph['node_types']\n",
    "            kept_indices = torch.where(node_keep_mask)[0].tolist()\n",
    "            new_node_types = [original_node_types[i] for i in kept_indices if i < len(original_node_types)]\n",
    "            \n",
    "            # Ensure we have the right number of node types\n",
    "            if len(new_node_types) != aug_nodes.size(0):\n",
    "                print(f\"    Warning: {pdb_id} - Node type mismatch: {len(new_node_types)} types, {aug_nodes.size(0)} nodes\")\n",
    "                # Pad or truncate as needed\n",
    "                if len(new_node_types) < aug_nodes.size(0):\n",
    "                    # Pad with the most common type\n",
    "                    from collections import Counter\n",
    "                    most_common = Counter(new_node_types).most_common(1)[0][0] if new_node_types else 'P'\n",
    "                    new_node_types.extend([most_common] * (aug_nodes.size(0) - len(new_node_types)))\n",
    "                else:\n",
    "                    new_node_types = new_node_types[:aug_nodes.size(0)]\n",
    "            \n",
    "            # Create synthetic graph\n",
    "            synthetic_graph = deepcopy(original_graph)\n",
    "            synthetic_graph['node_features'] = aug_nodes.cpu().numpy().tolist()\n",
    "            synthetic_graph['edge_index'] = aug_edges.cpu().numpy().tolist()\n",
    "            synthetic_graph['edge_features'] = aug_edge_attrs.cpu().numpy().tolist()\n",
    "            synthetic_graph['node_types'] = new_node_types\n",
    "            synthetic_graph['num_nodes'] = aug_nodes.size(0)\n",
    "            synthetic_graph['pdb_id'] = f\"{pdb_id}_synthetic\"\n",
    "            synthetic_graph['synthetic_method'] = 'Conservative_GAN_Augmentation'\n",
    "            synthetic_graph['combination'] = combination\n",
    "            \n",
    "            return synthetic_graph\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    GAN error for {pdb_id}: {e}\")\n",
    "            return None\n",
    "\n",
    "class ConservativeStructureAwareGAN:\n",
    "    def __init__(self, device='cuda'):\n",
    "        self.device = device\n",
    "        self.gan = ImprovedFastMolecularGraphGAN(device=device)\n",
    "        self.is_trained = False\n",
    "    \n",
    "    def train_on_combination(self, graph_batch, combination):\n",
    "        \"\"\"Train once per combination on batch of graphs\"\"\"\n",
    "        if not self.is_trained:\n",
    "            print(f\"    Training Conservative GAN for {combination}...\")\n",
    "            self.gan.batch_train(graph_batch[:50], epochs=5)  # Reduced epochs\n",
    "            self.is_trained = True\n",
    "    \n",
    "    def generate_synthetic_graph(self, original_graph, pdb_id, combination):\n",
    "        return self.gan.generate_synthetic_graph(original_graph, pdb_id, combination)\n",
    "\n",
    "def generate_synthetic_graphs_controlled(real_graph_dir, dataset_name, combination, output_dir):\n",
    "    \"\"\"Generate synthetic graphs using controlled augmentation (not GAN)\"\"\"\n",
    "    print(f\"  Generating synthetic graphs for {combination} using Controlled Augmentation...\")\n",
    "    \n",
    "    dataset_dir = os.path.join(real_graph_dir, dataset_name)\n",
    "    \n",
    "    if not os.path.exists(dataset_dir):\n",
    "        print(f\"    Dataset directory not found: {dataset_dir}\")\n",
    "        return 0\n",
    "    \n",
    "    pdb_dirs = [d for d in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir, d))]\n",
    "    print(f\"    Found {len(pdb_dirs)} PDB directories\")\n",
    "    \n",
    "    # Use the working StructureAwareGAN from your original code\n",
    "    augmenter = StructureAwareGAN()\n",
    "    \n",
    "    generated_count = 0\n",
    "    failed_count = 0\n",
    "    debug_failures = {\"no_graph\": 0, \"no_affinity\": 0, \"generation_failed\": 0, \"empty_result\": 0}\n",
    "    \n",
    "    for idx, pdb_dir in enumerate(pdb_dirs):\n",
    "        if idx % 500 == 0:\n",
    "            print(f\"    Progress: {idx}/{len(pdb_dirs)}\")\n",
    "            \n",
    "        original_path = os.path.join(dataset_dir, pdb_dir, f'{pdb_dir}_{combination}.pkl')\n",
    "        affinity_path = os.path.join(dataset_dir, pdb_dir, f'{pdb_dir}_affinity.pkl')\n",
    "        \n",
    "        if not os.path.exists(original_path):\n",
    "            debug_failures[\"no_graph\"] += 1\n",
    "            failed_count += 1\n",
    "            continue\n",
    "            \n",
    "        if not os.path.exists(affinity_path):\n",
    "            debug_failures[\"no_affinity\"] += 1\n",
    "            failed_count += 1\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Load original graph\n",
    "            with open(original_path, 'rb') as f:\n",
    "                original_graph = pickle.load(f)\n",
    "            \n",
    "            # Load affinity\n",
    "            with open(affinity_path, 'rb') as f:\n",
    "                affinity_data = pickle.load(f)\n",
    "            \n",
    "            # Check if graph is valid\n",
    "            if not original_graph.get('node_features') or len(original_graph['node_features']) < 3:\n",
    "                debug_failures[\"empty_result\"] += 1\n",
    "                failed_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Generate synthetic graph using controlled augmentation\n",
    "            synthetic_graph = augmenter.augment_molecular_graph(original_graph, pdb_dir, combination)\n",
    "            \n",
    "            if synthetic_graph and len(synthetic_graph.get('node_features', [])) > 0:\n",
    "                # Save synthetic graph\n",
    "                save_graph_data(synthetic_graph, output_dir, f\"{dataset_name}_synthetic\", affinity_data['affinity'])\n",
    "                generated_count += 1\n",
    "            else:\n",
    "                debug_failures[\"generation_failed\"] += 1\n",
    "                failed_count += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"    Error processing {pdb_dir}: {e}\")\n",
    "            debug_failures[\"generation_failed\"] += 1\n",
    "            failed_count += 1\n",
    "            continue\n",
    "    \n",
    "    print(f\"  {combination}: {generated_count} synthetic graphs generated, {failed_count} failed\")\n",
    "    print(f\"    Failure breakdown: {debug_failures}\")\n",
    "    return generated_count\n",
    "\n",
    "def main():\n",
    "    print(\"ðŸ”¬ COMPLETE GRAPH GENERATION PIPELINE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # File paths\n",
    "    real_train_csv = 'D:\\\\PhD\\\\Chapter_4\\\\Code2\\\\pdbbind\\\\pdb_ids_Affinity\\\\training_set_with_affinity.csv'\n",
    "    real_val_csv = 'D:\\\\PhD\\\\Chapter_4\\\\Code2\\\\pdbbind\\\\pdb_ids_Affinity\\\\validation_set_with_affinity.csv'\n",
    "    real_data_path = 'D:\\\\PhD\\\\Chapter_4\\\\Code2\\\\pdbbind\\\\dataset'\n",
    "    core_csv = 'D:\\\\PhD\\\\Chapter_4\\\\Code2\\\\pdbbind\\\\pdb_ids_Affinity\\\\core_set_with_affinity.csv'\n",
    "    holdout_csv = 'D:\\\\PhD\\\\Chapter_4\\\\Code2\\\\pdbbind\\\\pdb_ids_Affinity\\\\hold_out_set_with_affinity.csv'\n",
    "    \n",
    "    # Create timestamped output directory\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = f'D:\\\\PhD\\\\Chapter_4\\\\Code2\\\\pdbbind\\\\complete_graphs_{timestamp}'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    \n",
    "    # Load datasets\n",
    "    print(\"\\nðŸ“Š Loading datasets...\")\n",
    "    train_df = load_csv(real_train_csv, use_half=False)\n",
    "    val_df = load_csv(real_val_csv, use_half=False)\n",
    "    core_df = load_csv(core_csv, use_half=False)\n",
    "    holdout_df = load_csv(holdout_csv, use_half=False)\n",
    "    \n",
    "    print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Core: {len(core_df)}, Holdout: {len(holdout_df)}\")\n",
    "    \n",
    "    datasets = {\n",
    "        'training': train_df,\n",
    "        'validation': val_df,\n",
    "        'core': core_df,\n",
    "        'holdout': holdout_df\n",
    "    }\n",
    "    \n",
    "    combinations = ['P', 'L', 'I', 'PL', 'PI', 'LI', 'PLI']\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    # Process each combination\n",
    "    for combination in combinations:\n",
    "        print(f\"\\n{'='*30} {combination} {'='*30}\")\n",
    "        \n",
    "        # Step 1: Generate real graphs\n",
    "        print(\"ðŸ”§ Generating real graphs from JSON files...\")\n",
    "        for dataset_name, df in datasets.items():\n",
    "            if len(df) > 0:\n",
    "                prepare_real_graphs(df, real_data_path, combination, output_dir, dataset_name)\n",
    "        \n",
    "        # Step 2: Generate synthetic graphs\n",
    "        print(\"ðŸ¤– Generating synthetic graphs using augmentation...\")\n",
    "        for dataset_name in ['training', 'validation']:\n",
    "            generate_synthetic_graphs_controlled(output_dir, dataset_name, combination, output_dir)\n",
    "    \n",
    "    # Summary\n",
    "    total_time = time.time() - total_start_time\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"âœ… COMPLETE PIPELINE FINISHED!\")\n",
    "    print(f\"Total time: {total_time/60:.1f} minutes\")\n",
    "    print(f\"Output saved to: {output_dir}\")\n",
    "    \n",
    "    # Count files\n",
    "    for dataset_type in ['training', 'validation', 'core', 'holdout', 'training_synthetic', 'validation_synthetic']:\n",
    "        dataset_path = os.path.join(output_dir, dataset_type)\n",
    "        if os.path.exists(dataset_path):\n",
    "            count = len([d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))])\n",
    "            print(f\"{dataset_type}: {count} PDB directories\")\n",
    "    \n",
    "    print(f\"\\nðŸ“ Directory structure created:\")\n",
    "    print(f\"  {output_dir}/\")\n",
    "    print(f\"    â”œâ”€â”€ training/\")\n",
    "    print(f\"    â”œâ”€â”€ validation/\")\n",
    "    print(f\"    â”œâ”€â”€ core/\")\n",
    "    print(f\"    â”œâ”€â”€ holdout/\")\n",
    "    print(f\"    â”œâ”€â”€ training_synthetic/\")\n",
    "    print(f\"    â””â”€â”€ validation_synthetic/\")\n",
    "    print(f\"\\nEach PDB directory contains:\")\n",
    "    print(f\"  - {'{pdb_id}'}_{'{combination}'}.pkl (graph data)\")\n",
    "    print(f\"  - {'{pdb_id}'}_affinity.pkl (binding affinity)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb23f91-2a3d-4c49-9d4b-0b9e9029ae3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [torchfix]",
   "language": "python",
   "name": "torchfix"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
