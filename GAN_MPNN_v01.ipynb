{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3282efd8-e4ed-4459-b671-ae3dd334182f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¬ SIMPLE MPNN BASELINE (COMBINED REAL+SYNTHETIC)\n",
      "============================================================\n",
      "Loading test datasets...\n",
      "Core: 257, Holdout: 3393\n",
      "Using device: cuda\n",
      "\n",
      "==================== COMBINED P ====================\n",
      "Preparing combined dataset for P...\n",
      "  Real P: 9312 graphs loaded, 350 failed\n",
      "  Real P: 871 graphs loaded, 32 failed\n",
      "  Synthetic P: 9287 graphs loaded, 25 failed\n",
      "  Synthetic P: 871 graphs loaded, 0 failed\n",
      "  Train: 9312 real + 9287 synthetic = 18599\n",
      "  Val: 871 real + 871 synthetic = 1742\n",
      "Preparing test datasets...\n",
      "  Real P: 249 graphs loaded, 8 failed\n",
      "  Real P: 3232 graphs loaded, 161 failed\n",
      "Training model...\n",
      "    Epoch 0: Train Loss=4.7890, Val Loss=3.7244\n",
      "    Epoch 25: Train Loss=3.1310, Val Loss=3.5064\n",
      "    Epoch 50: Train Loss=3.0816, Val Loss=3.5660\n",
      "    Epoch 75: Train Loss=3.0931, Val Loss=3.4774\n",
      "Testing on 2016 core set...\n",
      "  Core Set - Rp: 0.232, RMSE: 2.012\n",
      "Testing on 2019 hold-out set...\n",
      "  Holdout Set - Rp: 0.261, RMSE: 1.711\n",
      "  Model saved: saved_models\\GAN_MPNN_baseline_models_20250709_185820\\model_P.pth\n",
      "\n",
      "==================== COMBINED L ====================\n",
      "Preparing combined dataset for L...\n",
      "  Real L: 9312 graphs loaded, 350 failed\n",
      "  Real L: 871 graphs loaded, 32 failed\n",
      "  Synthetic L: 9312 graphs loaded, 0 failed\n",
      "  Synthetic L: 871 graphs loaded, 0 failed\n",
      "  Train: 9312 real + 9312 synthetic = 18624\n",
      "  Val: 871 real + 871 synthetic = 1742\n",
      "Preparing test datasets...\n",
      "  Real L: 249 graphs loaded, 8 failed\n",
      "  Real L: 3232 graphs loaded, 161 failed\n",
      "Training model...\n",
      "    Epoch 0: Train Loss=5.1340, Val Loss=3.5093\n",
      "    Epoch 25: Train Loss=2.8758, Val Loss=3.2804\n",
      "    Epoch 50: Train Loss=2.7120, Val Loss=3.1364\n",
      "    Epoch 75: Train Loss=2.6690, Val Loss=3.1409\n",
      "Testing on 2016 core set...\n",
      "  Core Set - Rp: 0.430, RMSE: 1.852\n",
      "Testing on 2019 hold-out set...\n",
      "  Holdout Set - Rp: 0.390, RMSE: 1.618\n",
      "  Model saved: saved_models\\GAN_MPNN_baseline_models_20250709_190655\\model_L.pth\n",
      "\n",
      "==================== COMBINED I ====================\n",
      "Preparing combined dataset for I...\n",
      "  Real I: 9312 graphs loaded, 350 failed\n",
      "  Real I: 871 graphs loaded, 32 failed\n",
      "  Synthetic I: 9312 graphs loaded, 0 failed\n",
      "  Synthetic I: 871 graphs loaded, 0 failed\n",
      "  Train: 9312 real + 9312 synthetic = 18624\n",
      "  Val: 871 real + 871 synthetic = 1742\n",
      "Preparing test datasets...\n",
      "  Real I: 249 graphs loaded, 8 failed\n",
      "  Real I: 3232 graphs loaded, 161 failed\n",
      "Training model...\n",
      "    Epoch 0: Train Loss=4.4018, Val Loss=4.0024\n",
      "    Epoch 25: Train Loss=2.9158, Val Loss=3.3410\n",
      "    Epoch 50: Train Loss=2.8232, Val Loss=3.3187\n",
      "    Epoch 75: Train Loss=2.7756, Val Loss=3.4533\n",
      "Testing on 2016 core set...\n",
      "  Core Set - Rp: 0.455, RMSE: 1.850\n",
      "Testing on 2019 hold-out set...\n",
      "  Holdout Set - Rp: 0.382, RMSE: 1.627\n",
      "  Model saved: saved_models\\GAN_MPNN_baseline_models_20250709_191808\\model_I.pth\n",
      "\n",
      "==================== COMBINED PL ====================\n",
      "Preparing combined dataset for PL...\n",
      "  Real PL: 9312 graphs loaded, 350 failed\n",
      "  Real PL: 871 graphs loaded, 32 failed\n",
      "  Synthetic PL: 9312 graphs loaded, 0 failed\n",
      "  Synthetic PL: 871 graphs loaded, 0 failed\n",
      "  Train: 9312 real + 9312 synthetic = 18624\n",
      "  Val: 871 real + 871 synthetic = 1742\n",
      "Preparing test datasets...\n",
      "  Real PL: 249 graphs loaded, 8 failed\n",
      "  Real PL: 3232 graphs loaded, 161 failed\n",
      "Training model...\n",
      "    Epoch 0: Train Loss=4.8024, Val Loss=3.5503\n",
      "    Epoch 25: Train Loss=2.9226, Val Loss=3.3369\n",
      "    Epoch 50: Train Loss=2.8133, Val Loss=3.3293\n",
      "    Epoch 75: Train Loss=2.7262, Val Loss=3.1417\n",
      "Testing on 2016 core set...\n",
      "  Core Set - Rp: 0.462, RMSE: 1.822\n",
      "Testing on 2019 hold-out set...\n",
      "  Holdout Set - Rp: 0.362, RMSE: 1.634\n",
      "  Model saved: saved_models\\GAN_MPNN_baseline_models_20250709_192834\\model_PL.pth\n",
      "\n",
      "==================== COMBINED PI ====================\n",
      "Preparing combined dataset for PI...\n",
      "  Real PI: 9312 graphs loaded, 350 failed\n",
      "  Real PI: 871 graphs loaded, 32 failed\n",
      "  Synthetic PI: 9312 graphs loaded, 0 failed\n",
      "  Synthetic PI: 871 graphs loaded, 0 failed\n",
      "  Train: 9312 real + 9312 synthetic = 18624\n",
      "  Val: 871 real + 871 synthetic = 1742\n",
      "Preparing test datasets...\n",
      "  Real PI: 249 graphs loaded, 8 failed\n",
      "  Real PI: 3232 graphs loaded, 161 failed\n",
      "Training model...\n",
      "    Epoch 0: Train Loss=4.7948, Val Loss=3.5950\n",
      "    Epoch 25: Train Loss=2.9886, Val Loss=3.4492\n",
      "    Epoch 50: Train Loss=2.9636, Val Loss=3.4326\n",
      "    Epoch 75: Train Loss=2.9460, Val Loss=3.3843\n",
      "Testing on 2016 core set...\n",
      "  Core Set - Rp: 0.304, RMSE: 1.968\n",
      "Testing on 2019 hold-out set...\n",
      "  Holdout Set - Rp: 0.259, RMSE: 1.717\n",
      "  Model saved: saved_models\\GAN_MPNN_baseline_models_20250709_193801\\model_PI.pth\n",
      "\n",
      "==================== COMBINED LI ====================\n",
      "Preparing combined dataset for LI...\n",
      "  Real LI: 9312 graphs loaded, 350 failed\n",
      "  Real LI: 871 graphs loaded, 32 failed\n",
      "  Synthetic LI: 9312 graphs loaded, 0 failed\n",
      "  Synthetic LI: 871 graphs loaded, 0 failed\n",
      "  Train: 9312 real + 9312 synthetic = 18624\n",
      "  Val: 871 real + 871 synthetic = 1742\n",
      "Preparing test datasets...\n",
      "  Real LI: 249 graphs loaded, 8 failed\n",
      "  Real LI: 3232 graphs loaded, 161 failed\n",
      "Training model...\n",
      "    Epoch 0: Train Loss=4.6156, Val Loss=3.8461\n",
      "    Epoch 25: Train Loss=2.8826, Val Loss=3.3653\n",
      "    Epoch 50: Train Loss=2.7853, Val Loss=3.3095\n",
      "    Epoch 75: Train Loss=2.7179, Val Loss=3.2422\n",
      "Testing on 2016 core set...\n",
      "  Core Set - Rp: 0.444, RMSE: 1.868\n",
      "Testing on 2019 hold-out set...\n",
      "  Holdout Set - Rp: 0.367, RMSE: 1.653\n",
      "  Model saved: saved_models\\GAN_MPNN_baseline_models_20250709_194645\\model_LI.pth\n",
      "\n",
      "==================== COMBINED PLI ====================\n",
      "Preparing combined dataset for PLI...\n",
      "  Real PLI: 9312 graphs loaded, 350 failed\n",
      "  Real PLI: 871 graphs loaded, 32 failed\n",
      "  Synthetic PLI: 9312 graphs loaded, 0 failed\n",
      "  Synthetic PLI: 871 graphs loaded, 0 failed\n",
      "  Train: 9312 real + 9312 synthetic = 18624\n",
      "  Val: 871 real + 871 synthetic = 1742\n",
      "Preparing test datasets...\n",
      "  Real PLI: 249 graphs loaded, 8 failed\n",
      "  Real PLI: 3232 graphs loaded, 161 failed\n",
      "Training model...\n",
      "    Epoch 0: Train Loss=6.5541, Val Loss=3.8626\n",
      "    Epoch 25: Train Loss=2.8357, Val Loss=3.4346\n",
      "    Epoch 50: Train Loss=2.7750, Val Loss=3.4025\n",
      "    Epoch 75: Train Loss=2.7440, Val Loss=3.2135\n",
      "Testing on 2016 core set...\n",
      "  Core Set - Rp: 0.462, RMSE: 1.842\n",
      "Testing on 2019 hold-out set...\n",
      "  Holdout Set - Rp: 0.391, RMSE: 1.608\n",
      "  Model saved: saved_models\\GAN_MPNN_baseline_models_20250709_195922\\model_PLI.pth\n",
      "\n",
      "======================================================================\n",
      "COMBINED BASELINE RESULTS (Real + Synthetic)\n",
      "======================================================================\n",
      "Model  2016 core set             2019 hold-out set        \n",
      "       Rp           RMSE         Rp           RMSE        \n",
      "----------------------------------------------------------------------\n",
      "P      0.232        2.012        0.261        1.711       \n",
      "L      0.430        1.852        0.390        1.618       \n",
      "I      0.455        1.850        0.382        1.627       \n",
      "PL     0.462        1.822        0.362        1.634       \n",
      "PI     0.304        1.968        0.259        1.717       \n",
      "LI     0.444        1.868        0.367        1.653       \n",
      "PLI    0.462        1.842        0.391        1.608       \n",
      "======================================================================\n",
      "Note: Simple MPNN baseline with combined real+synthetic data\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import MessagePassing, global_mean_pool\n",
    "from torch_geometric.utils import add_self_loops, to_undirected\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Simplified atom property dictionary\n",
    "atom_property_dict = {\n",
    "    'H': {'atomic_num': 1, 'mass': 1.008, 'electronegativity': 2.20, 'vdw_radius': 1.20},\n",
    "    'C': {'atomic_num': 6, 'mass': 12.011, 'electronegativity': 2.55, 'vdw_radius': 1.70},\n",
    "    'N': {'atomic_num': 7, 'mass': 14.007, 'electronegativity': 3.04, 'vdw_radius': 1.55},\n",
    "    'O': {'atomic_num': 8, 'mass': 15.999, 'electronegativity': 3.44, 'vdw_radius': 1.52},\n",
    "    'P': {'atomic_num': 15, 'mass': 30.974, 'electronegativity': 2.19, 'vdw_radius': 1.80},\n",
    "    'S': {'atomic_num': 16, 'mass': 32.065, 'electronegativity': 2.58, 'vdw_radius': 1.80},\n",
    "    'F': {'atomic_num': 9, 'mass': 18.998, 'electronegativity': 3.98, 'vdw_radius': 1.47},\n",
    "    'Cl': {'atomic_num': 17, 'mass': 35.453, 'electronegativity': 3.16, 'vdw_radius': 1.75},\n",
    "    'Br': {'atomic_num': 35, 'mass': 79.904, 'electronegativity': 2.96, 'vdw_radius': 1.85},\n",
    "    'I': {'atomic_num': 53, 'mass': 126.904, 'electronegativity': 2.66, 'vdw_radius': 1.98},\n",
    "    'CA': {'atomic_num': 6, 'mass': 12.011, 'electronegativity': 2.55, 'vdw_radius': 1.70},\n",
    "    'CZ': {'atomic_num': 6, 'mass': 12.011, 'electronegativity': 2.55, 'vdw_radius': 1.70},\n",
    "    'OG': {'atomic_num': 8, 'mass': 15.999, 'electronegativity': 3.44, 'vdw_radius': 1.52},\n",
    "    'ZN': {'atomic_num': 30, 'mass': 65.38, 'electronegativity': 1.65, 'vdw_radius': 1.39},\n",
    "    'MG': {'atomic_num': 12, 'mass': 24.305, 'electronegativity': 1.31, 'vdw_radius': 1.73},\n",
    "    'FE': {'atomic_num': 26, 'mass': 55.845, 'electronegativity': 1.83, 'vdw_radius': 1.72},\n",
    "    'MN': {'atomic_num': 25, 'mass': 54.938, 'electronegativity': 1.55, 'vdw_radius': 1.73},\n",
    "    'CU': {'atomic_num': 29, 'mass': 63.546, 'electronegativity': 1.90, 'vdw_radius': 1.40},\n",
    "}\n",
    "\n",
    "def load_csv(csv_path):\n",
    "    \"\"\"Load CSV and sample half the data\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df[df['Affinity_pK'] != 0]\n",
    "    return df\n",
    "\n",
    "def create_basic_features(node, atom_property_dict):\n",
    "    \"\"\"Create basic atomic features (deliberately simple)\"\"\"\n",
    "    atom_type = node['attype']\n",
    "    prop = atom_property_dict.get(atom_type, \n",
    "                                 {'atomic_num': 6, 'mass': 12.011, 'electronegativity': 2.55, 'vdw_radius': 1.70})\n",
    "    \n",
    "    # Only basic features - no complex encoding\n",
    "    features = [\n",
    "        prop['atomic_num'],\n",
    "        prop['mass'],\n",
    "        prop['electronegativity'],\n",
    "        prop['vdw_radius']\n",
    "    ]\n",
    "    return features\n",
    "\n",
    "def load_single_graph(pdb_id, base_path, graph_type):\n",
    "    \"\"\"Load a single real graph with basic processing\"\"\"\n",
    "    if graph_type == 'P':\n",
    "        json_path = os.path.join(base_path, pdb_id, f'{pdb_id}_protein_graph.json')\n",
    "    elif graph_type == 'L':\n",
    "        json_path = os.path.join(base_path, pdb_id, f'{pdb_id}_ligand_graph.json')\n",
    "    elif graph_type == 'I':\n",
    "        json_path = os.path.join(base_path, pdb_id, f'{pdb_id}_interaction_graph.json')\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        with open(json_path, 'r') as file:\n",
    "            graph = json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        return None\n",
    "\n",
    "    if not graph['nodes']:\n",
    "        return None\n",
    "\n",
    "    # Create basic node features\n",
    "    node_features = []\n",
    "    for node in graph['nodes']:\n",
    "        features = create_basic_features(node, atom_property_dict)\n",
    "        node_features.append(features)\n",
    "\n",
    "    node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "    \n",
    "    # Basic edge processing\n",
    "    edge_index = []\n",
    "    for edge in graph['edges']:\n",
    "        if edge['id1'] is not None and edge['id2'] is not None:\n",
    "            edge_index.append([edge['id1'], edge['id2']])\n",
    "\n",
    "    if not edge_index:\n",
    "        num_nodes = len(node_features)\n",
    "        edge_index = torch.arange(num_nodes).unsqueeze(0).repeat(2, 1)\n",
    "    else:\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "        edge_index = to_undirected(edge_index)\n",
    "\n",
    "    return {\n",
    "        'node_features': node_features,\n",
    "        'edge_index': edge_index,\n",
    "        'num_nodes': len(node_features)\n",
    "    }\n",
    "\n",
    "def load_combined_graph(pdb_id, base_path, combination):\n",
    "    \"\"\"Load and combine real graphs with robust processing\"\"\"\n",
    "    graphs_to_load = []\n",
    "    \n",
    "    if 'P' in combination:\n",
    "        graphs_to_load.append('P')\n",
    "    if 'L' in combination:\n",
    "        graphs_to_load.append('L')\n",
    "    if 'I' in combination:\n",
    "        graphs_to_load.append('I')\n",
    "    \n",
    "    loaded_graphs = []\n",
    "    for graph_type in graphs_to_load:\n",
    "        graph = load_single_graph(pdb_id, base_path, graph_type)\n",
    "        loaded_graphs.append(graph)\n",
    "    \n",
    "    # Merge graphs\n",
    "    all_node_features = []\n",
    "    all_edge_indices = []\n",
    "    node_offset = 0\n",
    "    \n",
    "    for graph in loaded_graphs:\n",
    "        if graph is None:\n",
    "            continue\n",
    "            \n",
    "        all_node_features.append(graph['node_features'])\n",
    "        adjusted_edge_index = graph['edge_index'] + node_offset\n",
    "        all_edge_indices.append(adjusted_edge_index)\n",
    "        node_offset += graph['num_nodes']\n",
    "    \n",
    "    if not all_node_features:\n",
    "        return None\n",
    "    \n",
    "    merged_node_features = torch.cat(all_node_features, dim=0)\n",
    "    merged_edge_index = torch.cat(all_edge_indices, dim=1) if all_edge_indices else torch.empty((2, 0), dtype=torch.long)\n",
    "    \n",
    "    # Robust normalization to prevent NaN\n",
    "    if torch.isnan(merged_node_features).any() or torch.isinf(merged_node_features).any():\n",
    "        return None\n",
    "    \n",
    "    mean = merged_node_features.mean(dim=0, keepdim=True)\n",
    "    std = merged_node_features.std(dim=0, keepdim=True)\n",
    "    \n",
    "    # Prevent division by zero\n",
    "    std = torch.where(std < 1e-8, torch.ones_like(std), std)\n",
    "    merged_node_features = (merged_node_features - mean) / std\n",
    "    \n",
    "    # Clamp to prevent extreme values\n",
    "    merged_node_features = torch.clamp(merged_node_features, min=-10, max=10)\n",
    "    \n",
    "    merged_edge_index, _ = add_self_loops(merged_edge_index, num_nodes=merged_node_features.size(0))\n",
    "    \n",
    "    return Data(x=merged_node_features, edge_index=merged_edge_index)\n",
    "\n",
    "def prepare_real_dataset_combined(df, base_path, combination):\n",
    "    \"\"\"Prepare real dataset\"\"\"\n",
    "    data_list = []\n",
    "    failed_count = 0\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        pdb_id, affinity = row['PDB_ID'], row['Affinity_pK']\n",
    "        \n",
    "        if np.isnan(affinity) or np.isinf(affinity):\n",
    "            failed_count += 1\n",
    "            continue\n",
    "            \n",
    "        data = load_combined_graph(pdb_id, base_path, combination)\n",
    "        if data is not None:\n",
    "            data.y = torch.tensor([affinity], dtype=torch.float)\n",
    "            data.is_synthetic = False\n",
    "            data_list.append(data)\n",
    "        else:\n",
    "            failed_count += 1\n",
    "    \n",
    "    print(f\"  Real {combination}: {len(data_list)} graphs loaded, {failed_count} failed\")\n",
    "    return data_list\n",
    "\n",
    "def load_synthetic_graph_simple(pdb_id, synthetic_dir, combination):\n",
    "    \"\"\"Load synthetic graph with robust processing\"\"\"\n",
    "    graph_file = os.path.join(synthetic_dir, pdb_id, f'{pdb_id}_{combination}.pkl')\n",
    "    \n",
    "    if not os.path.exists(graph_file):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        with open(graph_file, 'rb') as f:\n",
    "            graph_data = pickle.load(f)\n",
    "        \n",
    "        # Extract basic features only\n",
    "        node_features = torch.tensor(graph_data['node_features'], dtype=torch.float)\n",
    "        edge_index = torch.tensor(graph_data['edge_index'], dtype=torch.long)\n",
    "        \n",
    "        # Basic validation\n",
    "        if torch.isnan(node_features).any() or torch.isinf(node_features).any():\n",
    "            return None\n",
    "        \n",
    "        # Simplify features to match real data format\n",
    "        if node_features.size(1) > 4:\n",
    "            node_features = node_features[:, :4]  # Take only first 4 features\n",
    "            # node_features = node_features\n",
    "        \n",
    "        # Ensure edge_index format\n",
    "        if edge_index.size(0) != 2:\n",
    "            edge_index = edge_index.t()\n",
    "        \n",
    "        # Filter invalid edges\n",
    "        if edge_index.size(1) > 0:\n",
    "            valid_edges = (edge_index[0] < node_features.size(0)) & (edge_index[1] < node_features.size(0))\n",
    "            edge_index = edge_index[:, valid_edges]\n",
    "        \n",
    "        # Handle empty edges\n",
    "        if edge_index.size(1) == 0:\n",
    "            num_nodes = node_features.size(0)\n",
    "            edge_index = torch.arange(num_nodes).unsqueeze(0).repeat(2, 1)\n",
    "        \n",
    "        # Robust normalization\n",
    "        if torch.isnan(node_features).any() or torch.isinf(node_features).any():\n",
    "            return None\n",
    "        \n",
    "        mean = node_features.mean(dim=0, keepdim=True)\n",
    "        std = node_features.std(dim=0, keepdim=True)\n",
    "        \n",
    "        # Prevent division by zero\n",
    "        std = torch.where(std < 1e-8, torch.ones_like(std), std)\n",
    "        node_features = (node_features - mean) / std\n",
    "        \n",
    "        # Clamp to prevent extreme values\n",
    "        node_features = torch.clamp(node_features, min=-10, max=10)\n",
    "        \n",
    "        # Add self-loops\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=node_features.size(0))\n",
    "        \n",
    "        return Data(x=node_features, edge_index=edge_index)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def load_affinity_data(pdb_id, synthetic_dir):\n",
    "    \"\"\"Load affinity data for synthetic graph\"\"\"\n",
    "    affinity_file = os.path.join(synthetic_dir, pdb_id, f'{pdb_id}_affinity.pkl')\n",
    "    \n",
    "    if not os.path.exists(affinity_file):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        with open(affinity_file, 'rb') as f:\n",
    "            affinity_data = pickle.load(f)\n",
    "        return affinity_data.get('affinity', None)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def prepare_synthetic_dataset_half(synthetic_dir, combination):\n",
    "    \"\"\"Prepare synthetic dataset with half data sampling\"\"\"\n",
    "    data_list = []\n",
    "    failed_count = 0\n",
    "    \n",
    "    if not os.path.exists(synthetic_dir):\n",
    "        print(f\"Synthetic directory not found: {synthetic_dir}\")\n",
    "        return []\n",
    "    \n",
    "    # Get all PDB directories and sample half\n",
    "    pdb_dirs = [d for d in os.listdir(synthetic_dir) if os.path.isdir(os.path.join(synthetic_dir, d))]\n",
    "    # pdb_dirs = np.random.choice(pdb_dirs, size=len(pdb_dirs)//2, replace=False).tolist()  # Half data\n",
    "    \n",
    "    for pdb_dir in pdb_dirs:\n",
    "        # Load affinity\n",
    "        affinity = load_affinity_data(pdb_dir, synthetic_dir)\n",
    "        if affinity is None or np.isnan(affinity) or np.isinf(affinity):\n",
    "            failed_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Load graph\n",
    "        data = load_synthetic_graph_simple(pdb_dir, synthetic_dir, combination)\n",
    "        if data is not None:\n",
    "            data.y = torch.tensor([affinity], dtype=torch.float)\n",
    "            data.is_synthetic = True\n",
    "            data_list.append(data)\n",
    "        else:\n",
    "            failed_count += 1\n",
    "    \n",
    "    print(f\"  Synthetic {combination}: {len(data_list)} graphs loaded, {failed_count} failed\")\n",
    "    return data_list\n",
    "\n",
    "def prepare_combined_training_dataset_half(real_train_csv, real_val_csv, real_data_path, \n",
    "                                          synthetic_train_dir, synthetic_val_dir, combination):\n",
    "    \"\"\"Combine real and synthetic datasets (half data)\"\"\"\n",
    "    print(f\"Preparing combined dataset for {combination}...\")\n",
    "    \n",
    "    # Load real data (already halved in load_csv)\n",
    "    real_train_df = load_csv(real_train_csv)\n",
    "    real_val_df = load_csv(real_val_csv)\n",
    "    \n",
    "    real_train_data = prepare_real_dataset_combined(real_train_df, real_data_path, combination)\n",
    "    real_val_data = prepare_real_dataset_combined(real_val_df, real_data_path, combination)\n",
    "    \n",
    "    # Load synthetic data (halved)\n",
    "    synthetic_train_data = prepare_synthetic_dataset_half(synthetic_train_dir, combination)\n",
    "    synthetic_val_data = prepare_synthetic_dataset_half(synthetic_val_dir, combination)\n",
    "    \n",
    "    # Combine\n",
    "    combined_train_data = real_train_data + synthetic_train_data\n",
    "    combined_val_data = real_val_data + synthetic_val_data\n",
    "    \n",
    "    print(f\"  Train: {len(real_train_data)} real + {len(synthetic_train_data)} synthetic = {len(combined_train_data)}\")\n",
    "    print(f\"  Val: {len(real_val_data)} real + {len(synthetic_val_data)} synthetic = {len(combined_val_data)}\")\n",
    "    \n",
    "    return combined_train_data, combined_val_data\n",
    "\n",
    "class SimpleMPNN(MessagePassing):\n",
    "    \"\"\"Simple MPNN layer\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(SimpleMPNN, self).__init__(aggr='mean')\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_channels * 2, out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(out_channels, out_channels)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        return self.propagate(edge_index, x=x)\n",
    "    \n",
    "    def message(self, x_i, x_j):\n",
    "        return self.mlp(torch.cat([x_i, x_j], dim=1))\n",
    "\n",
    "class SimpleCombinedBaseline(nn.Module):\n",
    "    \"\"\"Simple MPNN baseline for combined data\"\"\"\n",
    "    def __init__(self, input_dim=4, hidden_dim=64, num_layers=2):\n",
    "        super(SimpleCombinedBaseline, self).__init__()\n",
    "        \n",
    "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        self.mpnn_layers = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.mpnn_layers.append(SimpleMPNN(hidden_dim, hidden_dim))\n",
    "        \n",
    "        # Simple predictor\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.input_proj(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        for mpnn in self.mpnn_layers:\n",
    "            x = mpnn(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "        \n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.predictor(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def train_model_simple(model, train_loader, val_loader, epochs=100, device='cuda'):\n",
    "    \"\"\"Train model with robust setup\"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        valid_batches = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Check for NaN in input\n",
    "            if torch.isnan(batch.x).any() or torch.isnan(batch.y).any():\n",
    "                continue\n",
    "                \n",
    "            pred = model(batch.x, batch.edge_index, batch.batch).squeeze()\n",
    "            \n",
    "            # Check for NaN in prediction\n",
    "            if torch.isnan(pred).any():\n",
    "                continue\n",
    "                \n",
    "            loss = criterion(pred, batch.y)\n",
    "            \n",
    "            # Check for NaN in loss\n",
    "            if torch.isnan(loss):\n",
    "                continue\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            # Clip gradients to prevent explosion\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            valid_batches += 1\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_count = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch.to(device)\n",
    "                \n",
    "                # Check for NaN in input\n",
    "                if torch.isnan(batch.x).any() or torch.isnan(batch.y).any():\n",
    "                    continue\n",
    "                    \n",
    "                pred = model(batch.x, batch.edge_index, batch.batch).squeeze()\n",
    "                \n",
    "                # Check for NaN in prediction\n",
    "                if torch.isnan(pred).any():\n",
    "                    continue\n",
    "                    \n",
    "                loss = criterion(pred, batch.y)\n",
    "                \n",
    "                # Check for NaN in loss\n",
    "                if torch.isnan(loss):\n",
    "                    continue\n",
    "                    \n",
    "                val_loss += loss.item()\n",
    "                val_count += 1\n",
    "        \n",
    "        avg_val_loss = val_loss / max(val_count, 1)\n",
    "        avg_train_loss = total_loss / max(valid_batches, 1)\n",
    "        \n",
    "        if avg_val_loss < best_val_loss and not np.isnan(avg_val_loss):\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        \n",
    "        if epoch % 25 == 0:\n",
    "            print(f\"    Epoch {epoch}: Train Loss={avg_train_loss:.4f}, Val Loss={avg_val_loss:.4f}\")\n",
    "    \n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    return model\n",
    "\n",
    "def test_model(model, test_loader, device='cuda'):\n",
    "    \"\"\"Test model and return metrics\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(device)\n",
    "            pred = model(batch.x, batch.edge_index, batch.batch).squeeze()\n",
    "            \n",
    "            if pred.dim() == 0:\n",
    "                pred = pred.unsqueeze(0)\n",
    "            if batch.y.dim() == 0:\n",
    "                batch.y = batch.y.unsqueeze(0)\n",
    "            \n",
    "            predictions.extend(pred.cpu().numpy())\n",
    "            targets.extend(batch.y.cpu().numpy())\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    targets = np.array(targets)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    if len(predictions) > 1 and predictions.std() > 0.01:\n",
    "        rp, _ = pearsonr(predictions, targets)\n",
    "    else:\n",
    "        rp = 0.0\n",
    "    \n",
    "    rmse = np.sqrt(np.mean((predictions - targets) ** 2))\n",
    "    \n",
    "    return predictions, targets, rp, rmse\n",
    "\n",
    "def save_model_and_results(model, results, combination, save_dir=\"saved_models\"):\n",
    "    \"\"\"Save trained model and results\"\"\"\n",
    "    \n",
    "    # Create save directory with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_dir = os.path.join(save_dir, f\"GAN_MPNN_baseline_models_{timestamp}\")\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    # Save model state dict\n",
    "    model_path = os.path.join(model_dir, f\"model_{combination}.pth\")\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'combination': combination,\n",
    "        'results': results,\n",
    "        'model_config': {\n",
    "            'input_dim': 4,\n",
    "            'hidden_dim': 64,\n",
    "            'num_layers': 2\n",
    "        }\n",
    "    }, model_path)\n",
    "    \n",
    "    print(f\"  Model saved: {model_path}\")\n",
    "    return model_path\n",
    "\n",
    "def load_saved_model(model_path, device='cuda'):\n",
    "    \"\"\"Load a saved model\"\"\"\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    \n",
    "    # Recreate model with saved config\n",
    "    config = checkpoint['model_config']\n",
    "    model = SimpleBaseline(\n",
    "        input_dim=config['input_dim'],\n",
    "        hidden_dim=config['hidden_dim'],\n",
    "        num_layers=config['num_layers']\n",
    "    )\n",
    "    \n",
    "    # Load weights\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model = model.to(device)\n",
    "    \n",
    "    return model, checkpoint['results']\n",
    "\n",
    "def main():\n",
    "    print(\"ðŸ§¬ SIMPLE MPNN BASELINE (COMBINED REAL+SYNTHETIC)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # File paths - UPDATE THESE\n",
    "    real_train_csv = 'D:\\\\PhD\\\\Chapter_4\\\\Code2\\\\pdbbind\\\\pdb_ids_Affinity\\\\training_set_with_affinity.csv'\n",
    "    real_val_csv = 'D:\\\\PhD\\\\Chapter_4\\\\Code2\\\\pdbbind\\\\pdb_ids_Affinity\\\\validation_set_with_affinity.csv'\n",
    "    real_data_path = 'D:\\\\PhD\\\\Chapter_4\\\\Code2\\\\pdbbind\\\\dataset'\n",
    "    \n",
    "    synthetic_train_dir = 'D:\\\\PhD\\\\Chapter_4\\\\Code2\\\\pdbbind\\\\complete_graphs_20250709_163209\\\\training_synthetic'\n",
    "    synthetic_val_dir = 'D:\\\\PhD\\\\Chapter_4\\\\Code2\\\\pdbbind\\\\complete_graphs_20250709_163209\\\\validation_synthetic'\n",
    "    \n",
    "    # Test data paths\n",
    "    core_csv = 'D:\\\\PhD\\\\Chapter_4\\\\Code2\\\\pdbbind\\\\pdb_ids_Affinity\\\\core_set_with_affinity.csv'\n",
    "    holdout_csv = 'D:\\\\PhD\\\\Chapter_4\\\\Code2\\\\pdbbind\\\\pdb_ids_Affinity\\\\hold_out_set_with_affinity.csv'\n",
    "    \n",
    "    # Load test datasets (half data)\n",
    "    print(\"Loading test datasets...\")\n",
    "    core_df = load_csv(core_csv)\n",
    "    holdout_df = load_csv(holdout_csv)\n",
    "    \n",
    "    print(f\"Core: {len(core_df)}, Holdout: {len(holdout_df)}\")\n",
    "    \n",
    "    combinations = ['P', 'L', 'I', 'PL', 'PI', 'LI', 'PLI']\n",
    "    \n",
    "    # Device setup\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Results storage\n",
    "    results = {}\n",
    "    \n",
    "    for combination in combinations:\n",
    "        print(f\"\\n{'='*20} COMBINED {combination} {'='*20}\")\n",
    "        \n",
    "        # Prepare combined training datasets\n",
    "        combined_train_data, combined_val_data = prepare_combined_training_dataset_half(\n",
    "            real_train_csv, real_val_csv, real_data_path,\n",
    "            synthetic_train_dir, synthetic_val_dir, combination\n",
    "        )\n",
    "        \n",
    "        if len(combined_train_data) == 0 or len(combined_val_data) == 0:\n",
    "            print(f\"  Insufficient data for {combination}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Prepare test datasets\n",
    "        print(\"Preparing test datasets...\")\n",
    "        core_data = prepare_real_dataset_combined(core_df, real_data_path, combination)\n",
    "        holdout_data = prepare_real_dataset_combined(holdout_df, real_data_path, combination)\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(combined_train_data, batch_size=64, shuffle=True)\n",
    "        val_loader = DataLoader(combined_val_data, batch_size=64)\n",
    "        core_loader = DataLoader(core_data, batch_size=64) if core_data else None\n",
    "        holdout_loader = DataLoader(holdout_data, batch_size=64) if holdout_data else None\n",
    "        \n",
    "        # Train model\n",
    "        print(\"Training model...\")\n",
    "        input_dim = combined_train_data[0].x.size(1)\n",
    "        model = SimpleCombinedBaseline(input_dim=input_dim, hidden_dim=64, num_layers=2)\n",
    "        trained_model = train_model_simple(model, train_loader, val_loader, epochs=100, device=device)\n",
    "        \n",
    "        # Test on core set\n",
    "        if core_loader:\n",
    "            print(\"Testing on 2016 core set...\")\n",
    "            core_preds, core_targets, core_rp, core_rmse = test_model(trained_model, core_loader, device)\n",
    "            print(f\"  Core Set - Rp: {core_rp:.3f}, RMSE: {core_rmse:.3f}\")\n",
    "        else:\n",
    "            core_rp, core_rmse = 0, 0\n",
    "        \n",
    "        # Test on holdout set\n",
    "        if holdout_loader:\n",
    "            print(\"Testing on 2019 hold-out set...\")\n",
    "            holdout_preds, holdout_targets, holdout_rp, holdout_rmse = test_model(trained_model, holdout_loader, device)\n",
    "            print(f\"  Holdout Set - Rp: {holdout_rp:.3f}, RMSE: {holdout_rmse:.3f}\")\n",
    "        else:\n",
    "            holdout_rp, holdout_rmse = 0, 0\n",
    "        \n",
    "        # Store results\n",
    "        combination_results = {\n",
    "            'core_rp': core_rp,\n",
    "            'core_rmse': core_rmse,\n",
    "            'holdout_rp': holdout_rp,\n",
    "            'holdout_rmse': holdout_rmse\n",
    "        }\n",
    "        results[combination] = combination_results\n",
    "        \n",
    "        # Save model and results\n",
    "        save_model_and_results(trained_model, combination_results, combination)        \n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"COMBINED BASELINE RESULTS (Real + Synthetic)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"{'Model':<6} {'2016 core set':<25} {'2019 hold-out set':<25}\")\n",
    "    print(f\"{'':6} {'Rp':<12} {'RMSE':<12} {'Rp':<12} {'RMSE':<12}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for combination in combinations:\n",
    "        if combination in results:\n",
    "            r = results[combination]\n",
    "            print(f\"{combination:<6} {r['core_rp']:<12.3f} {r['core_rmse']:<12.3f} {r['holdout_rp']:<12.3f} {r['holdout_rmse']:<12.3f}\")\n",
    "    \n",
    "    print(f\"{'='*70}\")\n",
    "    print(\"Note: Simple MPNN baseline with combined real+synthetic data\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [torchfix]",
   "language": "python",
   "name": "torchfix"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
