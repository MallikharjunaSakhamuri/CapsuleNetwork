{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d592149-c674-43ec-ae57-f80ce1fc7289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¬ SIMPLE MPNN BASELINE\n",
      "==================================================\n",
      "Train: 9662, Val: 903, Core: 257, Holdout: 3393\n",
      "Using device: cuda\n",
      "\n",
      "==================== P ====================\n",
      "Preparing datasets...\n",
      "  P: 9312 graphs loaded, 350 failed\n",
      "  P: 871 graphs loaded, 32 failed\n",
      "  P: 249 graphs loaded, 8 failed\n",
      "  P: 3232 graphs loaded, 161 failed\n",
      "Training model...\n",
      "    Epoch 0: Train Loss=10.1079, Val Loss=4.5434\n",
      "    Epoch 25: Train Loss=2.9982, Val Loss=3.2938\n",
      "    Epoch 50: Train Loss=2.8631, Val Loss=3.1788\n",
      "    Epoch 75: Train Loss=2.8130, Val Loss=3.1431\n",
      "Testing on 2016 core set...\n",
      "  Core Set - Rp: 0.332, RMSE: 1.941\n",
      "Testing on 2019 hold-out set...\n",
      "  Holdout Set - Rp: 0.297, RMSE: 1.683\n",
      "  Model saved: saved_models\\Real_baseline_models_20250709_185308\\model_P.pth\n",
      "\n",
      "==================== L ====================\n",
      "Preparing datasets...\n",
      "  L: 9312 graphs loaded, 350 failed\n",
      "  L: 871 graphs loaded, 32 failed\n",
      "  L: 249 graphs loaded, 8 failed\n",
      "  L: 3232 graphs loaded, 161 failed\n",
      "Training model...\n",
      "    Epoch 0: Train Loss=7.0404, Val Loss=3.2563\n",
      "    Epoch 25: Train Loss=2.6484, Val Loss=3.0998\n",
      "    Epoch 50: Train Loss=2.5854, Val Loss=2.8843\n",
      "    Epoch 75: Train Loss=2.4694, Val Loss=2.8739\n",
      "Testing on 2016 core set...\n",
      "  Core Set - Rp: 0.479, RMSE: 1.827\n",
      "Testing on 2019 hold-out set...\n",
      "  Holdout Set - Rp: 0.449, RMSE: 1.569\n",
      "  Model saved: saved_models\\Real_baseline_models_20250709_185639\\model_L.pth\n",
      "\n",
      "==================== I ====================\n",
      "Preparing datasets...\n",
      "  I: 9312 graphs loaded, 350 failed\n",
      "  I: 871 graphs loaded, 32 failed\n",
      "  I: 249 graphs loaded, 8 failed\n",
      "  I: 3232 graphs loaded, 161 failed\n",
      "Training model...\n",
      "    Epoch 0: Train Loss=7.3604, Val Loss=4.0204\n",
      "    Epoch 25: Train Loss=2.7331, Val Loss=3.0791\n",
      "    Epoch 50: Train Loss=2.6454, Val Loss=2.9207\n",
      "    Epoch 75: Train Loss=2.5688, Val Loss=2.8789\n",
      "Testing on 2016 core set...\n",
      "  Core Set - Rp: 0.470, RMSE: 1.933\n",
      "Testing on 2019 hold-out set...\n",
      "  Holdout Set - Rp: 0.410, RMSE: 1.708\n",
      "  Model saved: saved_models\\Real_baseline_models_20250709_190206\\model_I.pth\n",
      "\n",
      "==================== PL ====================\n",
      "Preparing datasets...\n",
      "  PL: 9312 graphs loaded, 350 failed\n",
      "  PL: 871 graphs loaded, 32 failed\n",
      "  PL: 249 graphs loaded, 8 failed\n",
      "  PL: 3232 graphs loaded, 161 failed\n",
      "Training model...\n",
      "    Epoch 0: Train Loss=6.3791, Val Loss=3.4236\n",
      "    Epoch 25: Train Loss=2.9029, Val Loss=3.5825\n",
      "    Epoch 50: Train Loss=2.6083, Val Loss=3.3287\n",
      "    Epoch 75: Train Loss=2.5197, Val Loss=2.9870\n",
      "Testing on 2016 core set...\n",
      "  Core Set - Rp: 0.496, RMSE: 1.788\n",
      "Testing on 2019 hold-out set...\n",
      "  Holdout Set - Rp: 0.396, RMSE: 1.602\n",
      "  Model saved: saved_models\\Real_baseline_models_20250709_190803\\model_PL.pth\n",
      "\n",
      "==================== PI ====================\n",
      "Preparing datasets...\n",
      "  PI: 9312 graphs loaded, 350 failed\n",
      "  PI: 871 graphs loaded, 32 failed\n",
      "  PI: 249 graphs loaded, 8 failed\n",
      "  PI: 3232 graphs loaded, 161 failed\n",
      "Training model...\n",
      "    Epoch 0: Train Loss=6.2513, Val Loss=5.1851\n",
      "    Epoch 25: Train Loss=2.7402, Val Loss=3.0582\n",
      "    Epoch 50: Train Loss=2.6269, Val Loss=2.9503\n",
      "    Epoch 75: Train Loss=2.5049, Val Loss=2.7935\n",
      "Testing on 2016 core set...\n",
      "  Core Set - Rp: 0.464, RMSE: 1.831\n",
      "Testing on 2019 hold-out set...\n",
      "  Holdout Set - Rp: 0.366, RMSE: 1.672\n",
      "  Model saved: saved_models\\Real_baseline_models_20250709_191342\\model_PI.pth\n",
      "\n",
      "==================== LI ====================\n",
      "Preparing datasets...\n",
      "  LI: 9312 graphs loaded, 350 failed\n",
      "  LI: 871 graphs loaded, 32 failed\n",
      "  LI: 249 graphs loaded, 8 failed\n",
      "  LI: 3232 graphs loaded, 161 failed\n",
      "Training model...\n",
      "    Epoch 0: Train Loss=6.6326, Val Loss=3.9919\n",
      "    Epoch 25: Train Loss=2.7552, Val Loss=3.0839\n",
      "    Epoch 50: Train Loss=2.6015, Val Loss=2.9619\n",
      "    Epoch 75: Train Loss=2.5234, Val Loss=2.8803\n",
      "Testing on 2016 core set...\n",
      "  Core Set - Rp: 0.503, RMSE: 1.779\n",
      "Testing on 2019 hold-out set...\n",
      "  Holdout Set - Rp: 0.400, RMSE: 1.611\n",
      "  Model saved: saved_models\\Real_baseline_models_20250709_191855\\model_LI.pth\n",
      "\n",
      "==================== PLI ====================\n",
      "Preparing datasets...\n",
      "  PLI: 9312 graphs loaded, 350 failed\n",
      "  PLI: 871 graphs loaded, 32 failed\n",
      "  PLI: 249 graphs loaded, 8 failed\n",
      "  PLI: 3232 graphs loaded, 161 failed\n",
      "Training model...\n",
      "    Epoch 0: Train Loss=9.0046, Val Loss=3.6148\n",
      "    Epoch 25: Train Loss=2.8283, Val Loss=3.2241\n",
      "    Epoch 50: Train Loss=2.6857, Val Loss=3.0755\n",
      "    Epoch 75: Train Loss=2.5837, Val Loss=2.8939\n",
      "Testing on 2016 core set...\n",
      "  Core Set - Rp: 0.448, RMSE: 1.843\n",
      "Testing on 2019 hold-out set...\n",
      "  Holdout Set - Rp: 0.356, RMSE: 1.638\n",
      "  Model saved: saved_models\\Real_baseline_models_20250709_192510\\model_PLI.pth\n",
      "\n",
      "============================================================\n",
      "FINAL RESULTS (SIMPLE MPNN BASELINE)\n",
      "============================================================\n",
      "Model  2016 core set             2019 hold-out set        \n",
      "       Rp           RMSE         Rp           RMSE        \n",
      "------------------------------------------------------------\n",
      "P      0.332        1.941        0.297        1.683       \n",
      "L      0.479        1.827        0.449        1.569       \n",
      "I      0.470        1.933        0.410        1.708       \n",
      "PL     0.496        1.788        0.396        1.602       \n",
      "PI     0.464        1.831        0.366        1.672       \n",
      "LI     0.503        1.779        0.400        1.611       \n",
      "PLI    0.448        1.843        0.356        1.638       \n",
      "============================================================\n",
      "Note: This is a deliberately simple baseline model\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import MessagePassing, global_mean_pool\n",
    "from torch_geometric.utils import add_self_loops, to_undirected\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Simplified atom property dictionary\n",
    "atom_property_dict = {\n",
    "    'H': {'atomic_num': 1, 'mass': 1.008, 'electronegativity': 2.20, 'vdw_radius': 1.20},\n",
    "    'C': {'atomic_num': 6, 'mass': 12.011, 'electronegativity': 2.55, 'vdw_radius': 1.70},\n",
    "    'N': {'atomic_num': 7, 'mass': 14.007, 'electronegativity': 3.04, 'vdw_radius': 1.55},\n",
    "    'O': {'atomic_num': 8, 'mass': 15.999, 'electronegativity': 3.44, 'vdw_radius': 1.52},\n",
    "    'P': {'atomic_num': 15, 'mass': 30.974, 'electronegativity': 2.19, 'vdw_radius': 1.80},\n",
    "    'S': {'atomic_num': 16, 'mass': 32.065, 'electronegativity': 2.58, 'vdw_radius': 1.80},\n",
    "    'F': {'atomic_num': 9, 'mass': 18.998, 'electronegativity': 3.98, 'vdw_radius': 1.47},\n",
    "    'Cl': {'atomic_num': 17, 'mass': 35.453, 'electronegativity': 3.16, 'vdw_radius': 1.75},\n",
    "    'Br': {'atomic_num': 35, 'mass': 79.904, 'electronegativity': 2.96, 'vdw_radius': 1.85},\n",
    "    'I': {'atomic_num': 53, 'mass': 126.904, 'electronegativity': 2.66, 'vdw_radius': 1.98},\n",
    "    'CA': {'atomic_num': 6, 'mass': 12.011, 'electronegativity': 2.55, 'vdw_radius': 1.70},\n",
    "    'CZ': {'atomic_num': 6, 'mass': 12.011, 'electronegativity': 2.55, 'vdw_radius': 1.70},\n",
    "    'OG': {'atomic_num': 8, 'mass': 15.999, 'electronegativity': 3.44, 'vdw_radius': 1.52},\n",
    "    'ZN': {'atomic_num': 30, 'mass': 65.38, 'electronegativity': 1.65, 'vdw_radius': 1.39},\n",
    "    'MG': {'atomic_num': 12, 'mass': 24.305, 'electronegativity': 1.31, 'vdw_radius': 1.73},\n",
    "    'FE': {'atomic_num': 26, 'mass': 55.845, 'electronegativity': 1.83, 'vdw_radius': 1.72},\n",
    "    'MN': {'atomic_num': 25, 'mass': 54.938, 'electronegativity': 1.55, 'vdw_radius': 1.73},\n",
    "    'CU': {'atomic_num': 29, 'mass': 63.546, 'electronegativity': 1.90, 'vdw_radius': 1.40},\n",
    "}\n",
    "\n",
    "def load_csv(csv_path):\n",
    "    \"\"\"Load CSV data\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df[df['Affinity_pK'] != 0]\n",
    "    return df\n",
    "\n",
    "def create_basic_features(node, atom_property_dict):\n",
    "    \"\"\"Create basic atomic features (deliberately simple)\"\"\"\n",
    "    atom_type = node['attype']\n",
    "    prop = atom_property_dict.get(atom_type, \n",
    "                                 {'atomic_num': 6, 'mass': 12.011, 'electronegativity': 2.55, 'vdw_radius': 1.70})\n",
    "    \n",
    "    # Only basic features - no complex encoding\n",
    "    features = [\n",
    "        prop['atomic_num'],\n",
    "        prop['mass'],\n",
    "        prop['electronegativity'],\n",
    "        prop['vdw_radius']\n",
    "    ]\n",
    "    return features\n",
    "\n",
    "def load_single_graph(pdb_id, base_path, graph_type):\n",
    "    \"\"\"Load a single graph with basic processing\"\"\"\n",
    "    if graph_type == 'P':\n",
    "        json_path = os.path.join(base_path, pdb_id, f'{pdb_id}_protein_graph.json')\n",
    "    elif graph_type == 'L':\n",
    "        json_path = os.path.join(base_path, pdb_id, f'{pdb_id}_ligand_graph.json')\n",
    "    elif graph_type == 'I':\n",
    "        json_path = os.path.join(base_path, pdb_id, f'{pdb_id}_interaction_graph.json')\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        with open(json_path, 'r') as file:\n",
    "            graph = json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        return None\n",
    "\n",
    "    if not graph['nodes']:\n",
    "        return None\n",
    "\n",
    "    # Create basic node features\n",
    "    node_features = []\n",
    "    for node in graph['nodes']:\n",
    "        features = create_basic_features(node, atom_property_dict)\n",
    "        node_features.append(features)\n",
    "\n",
    "    node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "    \n",
    "    # Basic edge processing - no complex features\n",
    "    edge_index = []\n",
    "    for edge in graph['edges']:\n",
    "        if edge['id1'] is not None and edge['id2'] is not None:\n",
    "            edge_index.append([edge['id1'], edge['id2']])\n",
    "\n",
    "    if not edge_index:\n",
    "        num_nodes = len(node_features)\n",
    "        edge_index = torch.arange(num_nodes).unsqueeze(0).repeat(2, 1)\n",
    "    else:\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "        edge_index = to_undirected(edge_index)\n",
    "\n",
    "    return {\n",
    "        'node_features': node_features,\n",
    "        'edge_index': edge_index,\n",
    "        'num_nodes': len(node_features)\n",
    "    }\n",
    "\n",
    "def load_combined_graph(pdb_id, base_path, combination):\n",
    "    \"\"\"Load and combine graphs with minimal processing\"\"\"\n",
    "    graphs_to_load = []\n",
    "    \n",
    "    if 'P' in combination:\n",
    "        graphs_to_load.append('P')\n",
    "    if 'L' in combination:\n",
    "        graphs_to_load.append('L')\n",
    "    if 'I' in combination:\n",
    "        graphs_to_load.append('I')\n",
    "    \n",
    "    loaded_graphs = []\n",
    "    for graph_type in graphs_to_load:\n",
    "        graph = load_single_graph(pdb_id, base_path, graph_type)\n",
    "        loaded_graphs.append(graph)\n",
    "    \n",
    "    # Merge graphs\n",
    "    all_node_features = []\n",
    "    all_edge_indices = []\n",
    "    node_offset = 0\n",
    "    \n",
    "    for graph in loaded_graphs:\n",
    "        if graph is None:\n",
    "            continue\n",
    "            \n",
    "        all_node_features.append(graph['node_features'])\n",
    "        adjusted_edge_index = graph['edge_index'] + node_offset\n",
    "        all_edge_indices.append(adjusted_edge_index)\n",
    "        node_offset += graph['num_nodes']\n",
    "    \n",
    "    if not all_node_features:\n",
    "        return None\n",
    "    \n",
    "    merged_node_features = torch.cat(all_node_features, dim=0)\n",
    "    merged_edge_index = torch.cat(all_edge_indices, dim=1) if all_edge_indices else torch.empty((2, 0), dtype=torch.long)\n",
    "    \n",
    "    # Robust normalization to prevent NaN\n",
    "    if torch.isnan(merged_node_features).any() or torch.isinf(merged_node_features).any():\n",
    "        return None\n",
    "    \n",
    "    mean = merged_node_features.mean(dim=0, keepdim=True)\n",
    "    std = merged_node_features.std(dim=0, keepdim=True)\n",
    "    \n",
    "    # Prevent division by zero\n",
    "    std = torch.where(std < 1e-8, torch.ones_like(std), std)\n",
    "    merged_node_features = (merged_node_features - mean) / std\n",
    "    \n",
    "    # Clamp to prevent extreme values\n",
    "    merged_node_features = torch.clamp(merged_node_features, min=-10, max=10)\n",
    "    \n",
    "    merged_edge_index, _ = add_self_loops(merged_edge_index, num_nodes=merged_node_features.size(0))\n",
    "    \n",
    "    return Data(x=merged_node_features, edge_index=merged_edge_index)\n",
    "\n",
    "def prepare_dataset_combined(df, base_path, combination):\n",
    "    \"\"\"Prepare dataset for a specific graph combination\"\"\"\n",
    "    data_list = []\n",
    "    failed_count = 0\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        pdb_id, affinity = row['PDB_ID'], row['Affinity_pK']\n",
    "        \n",
    "        if np.isnan(affinity) or np.isinf(affinity):\n",
    "            failed_count += 1\n",
    "            continue\n",
    "            \n",
    "        data = load_combined_graph(pdb_id, base_path, combination)\n",
    "        if data is not None:\n",
    "            data.y = torch.tensor([affinity], dtype=torch.float)\n",
    "            data_list.append(data)\n",
    "        else:\n",
    "            failed_count += 1\n",
    "    \n",
    "    print(f\"  {combination}: {len(data_list)} graphs loaded, {failed_count} failed\")\n",
    "    return data_list\n",
    "\n",
    "class SimpleMPNN(MessagePassing):\n",
    "    \"\"\"Simple MPNN layer\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(SimpleMPNN, self).__init__(aggr='mean')\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_channels * 2, out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(out_channels, out_channels)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        return self.propagate(edge_index, x=x)\n",
    "    \n",
    "    def message(self, x_i, x_j):\n",
    "        return self.mlp(torch.cat([x_i, x_j], dim=1))\n",
    "\n",
    "class SimpleBaseline(nn.Module):\n",
    "    \"\"\"Simple MPNN baseline model\"\"\"\n",
    "    def __init__(self, input_dim=4, hidden_dim=64, num_layers=2):\n",
    "        super(SimpleBaseline, self).__init__()\n",
    "        \n",
    "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        self.mpnn_layers = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.mpnn_layers.append(SimpleMPNN(hidden_dim, hidden_dim))\n",
    "        \n",
    "        # Simple predictor\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.input_proj(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        for mpnn in self.mpnn_layers:\n",
    "            x = mpnn(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "        \n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.predictor(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=100, device='cuda'):\n",
    "    \"\"\"Train the model with basic setup\"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)  # Higher learning rate\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Check for NaN in input\n",
    "            if torch.isnan(batch.x).any() or torch.isnan(batch.y).any():\n",
    "                continue\n",
    "                \n",
    "            pred = model(batch.x, batch.edge_index, batch.batch).squeeze()\n",
    "            \n",
    "            # Check for NaN in prediction\n",
    "            if torch.isnan(pred).any():\n",
    "                continue\n",
    "                \n",
    "            loss = criterion(pred, batch.y)\n",
    "            \n",
    "            # Check for NaN in loss\n",
    "            if torch.isnan(loss):\n",
    "                continue\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            # Clip gradients to prevent explosion\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_count = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch.to(device)\n",
    "                \n",
    "                # Check for NaN in input\n",
    "                if torch.isnan(batch.x).any() or torch.isnan(batch.y).any():\n",
    "                    continue\n",
    "                    \n",
    "                pred = model(batch.x, batch.edge_index, batch.batch).squeeze()\n",
    "                \n",
    "                # Check for NaN in prediction\n",
    "                if torch.isnan(pred).any():\n",
    "                    continue\n",
    "                    \n",
    "                loss = criterion(pred, batch.y)\n",
    "                \n",
    "                # Check for NaN in loss\n",
    "                if torch.isnan(loss):\n",
    "                    continue\n",
    "                    \n",
    "                val_loss += loss.item()\n",
    "                val_count += 1\n",
    "        \n",
    "        avg_val_loss = val_loss / max(val_count, 1)\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        \n",
    "        if avg_val_loss < best_val_loss and not np.isnan(avg_val_loss):\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        \n",
    "        if epoch % 25 == 0:\n",
    "            print(f\"    Epoch {epoch}: Train Loss={avg_train_loss:.4f}, Val Loss={avg_val_loss:.4f}\")\n",
    "    \n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    return model\n",
    "\n",
    "def test_model(model, test_loader, device='cuda'):\n",
    "    \"\"\"Test the model and return predictions, targets, and metrics\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(device)\n",
    "            pred = model(batch.x, batch.edge_index, batch.batch).squeeze()\n",
    "            \n",
    "            if pred.dim() == 0:\n",
    "                pred = pred.unsqueeze(0)\n",
    "            if batch.y.dim() == 0:\n",
    "                batch.y = batch.y.unsqueeze(0)\n",
    "            \n",
    "            predictions.extend(pred.cpu().numpy())\n",
    "            targets.extend(batch.y.cpu().numpy())\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    targets = np.array(targets)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    if len(predictions) > 1 and predictions.std() > 0.01:\n",
    "        rp, _ = pearsonr(predictions, targets)\n",
    "    else:\n",
    "        rp = 0.0\n",
    "    \n",
    "    rmse = np.sqrt(np.mean((predictions - targets) ** 2))\n",
    "    \n",
    "    return predictions, targets, rp, rmse\n",
    "\n",
    "\n",
    "def save_model_and_results(model, results, combination, save_dir=\"saved_models\"):\n",
    "    \"\"\"Save trained model and results\"\"\"\n",
    "    \n",
    "    # Create save directory with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_dir = os.path.join(save_dir, f\"Real_baseline_models_{timestamp}\")\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    # Save model state dict\n",
    "    model_path = os.path.join(model_dir, f\"model_{combination}.pth\")\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'combination': combination,\n",
    "        'results': results,\n",
    "        'model_config': {\n",
    "            'input_dim': 4,\n",
    "            'hidden_dim': 64,\n",
    "            'num_layers': 2\n",
    "        }\n",
    "    }, model_path)\n",
    "    \n",
    "    print(f\"  Model saved: {model_path}\")\n",
    "    return model_path\n",
    "\n",
    "def load_saved_model(model_path, device='cuda'):\n",
    "    \"\"\"Load a saved model\"\"\"\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    \n",
    "    # Recreate model with saved config\n",
    "    config = checkpoint['model_config']\n",
    "    model = SimpleBaseline(\n",
    "        input_dim=config['input_dim'],\n",
    "        hidden_dim=config['hidden_dim'],\n",
    "        num_layers=config['num_layers']\n",
    "    )\n",
    "    \n",
    "    # Load weights\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model = model.to(device)\n",
    "    \n",
    "    return model, checkpoint['results']\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"ðŸ§¬ SIMPLE MPNN BASELINE\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # File paths - UPDATE THESE PATHS\n",
    "    train_csv = 'D:\\\\PhD\\\\Chapter_4\\\\Code2\\\\pdbbind\\\\pdb_ids_Affinity\\\\training_set_with_affinity.csv'\n",
    "    val_csv = 'D:\\\\PhD\\\\Chapter_4\\\\Code2\\\\pdbbind\\\\pdb_ids_Affinity\\\\validation_set_with_affinity.csv'\n",
    "    core_csv = 'D:\\\\PhD\\\\Chapter_4\\\\Code2\\\\pdbbind\\\\pdb_ids_Affinity\\\\core_set_with_affinity.csv'\n",
    "    holdout_csv = 'D:\\\\PhD\\\\Chapter_4\\\\Code2\\\\pdbbind\\\\pdb_ids_Affinity\\\\hold_out_set_with_affinity.csv'\n",
    "    base_path = 'D:\\\\PhD\\\\Chapter_4\\\\Code2\\\\pdbbind\\\\dataset'\n",
    "    \n",
    "    # Load datasets \n",
    "    train_df = load_csv(train_csv)\n",
    "    val_df = load_csv(val_csv)\n",
    "    core_df = load_csv(core_csv)\n",
    "    holdout_df = load_csv(holdout_csv)\n",
    "    \n",
    "    print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Core: {len(core_df)}, Holdout: {len(holdout_df)}\")\n",
    "    \n",
    "    # Test only one combination for baseline\n",
    "    # combinations = ['PL']  # Only protein-ligand combination\n",
    "    combinations = ['P', 'L', 'I', 'PL', 'PI', 'LI', 'PLI']\n",
    "    \n",
    "    # Device setup\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Results storage\n",
    "    results = {}\n",
    "    \n",
    "    # Process each combination\n",
    "    for combination in combinations:\n",
    "        print(f\"\\n{'='*20} {combination} {'='*20}\")\n",
    "        \n",
    "        # Prepare datasets\n",
    "        print(\"Preparing datasets...\")\n",
    "        train_data = prepare_dataset_combined(train_df, base_path, combination)\n",
    "        val_data = prepare_dataset_combined(val_df, base_path, combination)\n",
    "        core_data = prepare_dataset_combined(core_df, base_path, combination)\n",
    "        holdout_data = prepare_dataset_combined(holdout_df, base_path, combination)\n",
    "        \n",
    "        if len(train_data) == 0 or len(val_data) == 0:\n",
    "            print(f\"  Insufficient data for {combination}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "        val_loader = DataLoader(val_data, batch_size=64)\n",
    "        core_loader = DataLoader(core_data, batch_size=64) if core_data else None\n",
    "        holdout_loader = DataLoader(holdout_data, batch_size=64) if holdout_data else None\n",
    "        \n",
    "        # Train model\n",
    "        print(\"Training model...\")\n",
    "        model = SimpleBaseline(input_dim=4, hidden_dim=64, num_layers=2)\n",
    "        trained_model = train_model(model, train_loader, val_loader, epochs=100, device=device)\n",
    "        \n",
    "        # Test on core set\n",
    "        if core_loader:\n",
    "            print(\"Testing on 2016 core set...\")\n",
    "            core_preds, core_targets, core_rp, core_rmse = test_model(trained_model, core_loader, device)\n",
    "            print(f\"  Core Set - Rp: {core_rp:.3f}, RMSE: {core_rmse:.3f}\")\n",
    "        else:\n",
    "            core_rp, core_rmse = 0, 0\n",
    "        \n",
    "        # Test on holdout set\n",
    "        if holdout_loader:\n",
    "            print(\"Testing on 2019 hold-out set...\")\n",
    "            holdout_preds, holdout_targets, holdout_rp, holdout_rmse = test_model(trained_model, holdout_loader, device)\n",
    "            print(f\"  Holdout Set - Rp: {holdout_rp:.3f}, RMSE: {holdout_rmse:.3f}\")\n",
    "        else:\n",
    "            holdout_rp, holdout_rmse = 0, 0\n",
    "        \n",
    "        # Store results\n",
    "        combination_results = {\n",
    "            'core_rp': core_rp,\n",
    "            'core_rmse': core_rmse,\n",
    "            'holdout_rp': holdout_rp,\n",
    "            'holdout_rmse': holdout_rmse\n",
    "        }\n",
    "        results[combination] = combination_results\n",
    "        \n",
    "        # Save model and results\n",
    "        save_model_and_results(trained_model, combination_results, combination)\n",
    "    \n",
    "    # Print final results\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"FINAL RESULTS (SIMPLE MPNN BASELINE)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"{'Model':<6} {'2016 core set':<25} {'2019 hold-out set':<25}\")\n",
    "    print(f\"{'':6} {'Rp':<12} {'RMSE':<12} {'Rp':<12} {'RMSE':<12}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for combination in combinations:\n",
    "        if combination in results:\n",
    "            r = results[combination]\n",
    "            print(f\"{combination:<6} {r['core_rp']:<12.3f} {r['core_rmse']:<12.3f} {r['holdout_rp']:<12.3f} {r['holdout_rmse']:<12.3f}\")\n",
    "    \n",
    "    print(f\"{'='*60}\")\n",
    "    print(\"Note: This is a deliberately simple baseline model\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b771368c-1171-4bd0-9c8b-833b0ee2920a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [torchfix]",
   "language": "python",
   "name": "torchfix"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
